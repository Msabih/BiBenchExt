{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.3235, -0.1142, -0.2569],\n",
      "          [ 0.3151,  0.1815, -0.0572],\n",
      "          [ 0.1813,  0.2240, -0.0657]]],\n",
      "\n",
      "\n",
      "        [[[-0.0246, -0.1054,  0.0211],\n",
      "          [-0.3312,  0.2878,  0.0015],\n",
      "          [ 0.2827,  0.1188, -0.0476]]],\n",
      "\n",
      "\n",
      "        [[[-0.0374, -0.1837,  0.1866],\n",
      "          [-0.1761, -0.3094, -0.2416],\n",
      "          [-0.1913, -0.2855,  0.1398]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2311,  0.1743,  0.0419],\n",
      "          [ 0.3215, -0.0928, -0.0852],\n",
      "          [-0.0517, -0.1484, -0.1222]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2716,  0.0569, -0.0018],\n",
      "          [-0.0613,  0.1806, -0.1027],\n",
      "          [-0.0942, -0.0720,  0.2632]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1584,  0.2487,  0.3259],\n",
      "          [ 0.3252,  0.1288, -0.1190],\n",
      "          [ 0.2233, -0.0473,  0.1669]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1816,  0.0632,  0.0724],\n",
      "          [-0.2809,  0.1243,  0.0291],\n",
      "          [-0.1784, -0.2751,  0.0555]]],\n",
      "\n",
      "\n",
      "        [[[-0.0788,  0.1377, -0.0208],\n",
      "          [-0.2186,  0.2356, -0.0669],\n",
      "          [ 0.0473,  0.1626, -0.1805]]],\n",
      "\n",
      "\n",
      "        [[[-0.3285, -0.0145,  0.2316],\n",
      "          [-0.2537,  0.2698, -0.0432],\n",
      "          [ 0.1186, -0.0629, -0.3256]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1889,  0.0120,  0.0672],\n",
      "          [-0.2574, -0.1731,  0.0801],\n",
      "          [-0.2459, -0.0506,  0.1625]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0855,  0.1681, -0.2821],\n",
      "          [-0.0756, -0.3086, -0.1199],\n",
      "          [-0.2632,  0.2178, -0.2991]]],\n",
      "\n",
      "\n",
      "        [[[-0.1649,  0.0328, -0.0253],\n",
      "          [-0.2675,  0.3214,  0.3028],\n",
      "          [ 0.1190,  0.1008,  0.2534]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0841,  0.2481,  0.2028],\n",
      "          [-0.2289,  0.1786, -0.1508],\n",
      "          [ 0.2911, -0.3168, -0.2196]]],\n",
      "\n",
      "\n",
      "        [[[-0.1862,  0.3222,  0.2022],\n",
      "          [-0.1670,  0.1031,  0.1686],\n",
      "          [-0.1710,  0.1548, -0.1182]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2183,  0.2826,  0.0252],\n",
      "          [-0.1634, -0.3173, -0.1976],\n",
      "          [-0.0404,  0.1100,  0.0508]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3052,  0.1642, -0.0988],\n",
      "          [-0.0080,  0.0972,  0.0989],\n",
      "          [-0.1148,  0.2142,  0.2540]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1998, -0.2440, -0.0107],\n",
      "          [-0.2111,  0.2436, -0.2327],\n",
      "          [ 0.1090, -0.1804, -0.1528]]],\n",
      "\n",
      "\n",
      "        [[[-0.2459, -0.1379, -0.3146],\n",
      "          [ 0.0413, -0.2248,  0.1809],\n",
      "          [-0.0172, -0.1926,  0.1933]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2279,  0.0319, -0.2268],\n",
      "          [ 0.1851,  0.2203,  0.2925],\n",
      "          [-0.2541,  0.1188, -0.2272]]],\n",
      "\n",
      "\n",
      "        [[[-0.0730, -0.2732, -0.2006],\n",
      "          [ 0.1877,  0.0259, -0.2878],\n",
      "          [-0.2799,  0.0674, -0.2044]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2396,  0.2376, -0.0814],\n",
      "          [-0.2518, -0.2044,  0.2147],\n",
      "          [-0.2341, -0.1117,  0.0421]]],\n",
      "\n",
      "\n",
      "        [[[-0.1412, -0.0806,  0.1196],\n",
      "          [ 0.1939, -0.0148, -0.3010],\n",
      "          [ 0.3095, -0.3060, -0.0831]]],\n",
      "\n",
      "\n",
      "        [[[-0.0635, -0.2597, -0.2454],\n",
      "          [-0.1200, -0.2340,  0.2030],\n",
      "          [ 0.0267,  0.1660,  0.1985]]],\n",
      "\n",
      "\n",
      "        [[[-0.1953, -0.0704,  0.2311],\n",
      "          [-0.2198, -0.0083,  0.1182],\n",
      "          [ 0.2129, -0.2721,  0.2209]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1365, -0.2984, -0.2591],\n",
      "          [ 0.1757,  0.2469,  0.2434],\n",
      "          [ 0.3261,  0.0168, -0.1077]]],\n",
      "\n",
      "\n",
      "        [[[-0.0459,  0.0966,  0.1118],\n",
      "          [-0.1430,  0.0851,  0.3198],\n",
      "          [-0.1317, -0.1333,  0.2958]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2530,  0.0067,  0.3205],\n",
      "          [ 0.2255,  0.0310, -0.1223],\n",
      "          [ 0.2273,  0.2064,  0.1165]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0482, -0.1373,  0.1308],\n",
      "          [ 0.1815,  0.2759,  0.1364],\n",
      "          [-0.0664,  0.1949, -0.1556]]],\n",
      "\n",
      "\n",
      "        [[[-0.2620,  0.2112, -0.1176],\n",
      "          [-0.3106,  0.0733, -0.2679],\n",
      "          [ 0.1786, -0.1791,  0.3311]]],\n",
      "\n",
      "\n",
      "        [[[-0.1626, -0.0280, -0.2555],\n",
      "          [-0.1931,  0.3286,  0.0764],\n",
      "          [-0.3215,  0.1154, -0.0186]]],\n",
      "\n",
      "\n",
      "        [[[-0.2318, -0.2063,  0.0445],\n",
      "          [-0.2858, -0.0237, -0.2105],\n",
      "          [ 0.1133,  0.0485, -0.0118]]],\n",
      "\n",
      "\n",
      "        [[[-0.2731, -0.0512, -0.2686],\n",
      "          [-0.0373,  0.2360, -0.1450],\n",
      "          [ 0.1862,  0.1467,  0.0064]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2352,  0.2035, -0.2361],\n",
      "          [ 0.2886,  0.1427,  0.0881],\n",
      "          [ 0.1054,  0.0111,  0.2207]]],\n",
      "\n",
      "\n",
      "        [[[-0.1950, -0.1056, -0.0792],\n",
      "          [ 0.2666, -0.0677, -0.3141],\n",
      "          [ 0.3129,  0.3194, -0.1589]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2446, -0.1306, -0.2879],\n",
      "          [-0.1640,  0.0218, -0.3190],\n",
      "          [-0.2184,  0.0441,  0.2243]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0291, -0.3155, -0.3178],\n",
      "          [-0.1948,  0.2194, -0.0278],\n",
      "          [ 0.0263, -0.1286,  0.3080]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2036,  0.2947,  0.1184],\n",
      "          [ 0.1142,  0.2892, -0.3223],\n",
      "          [ 0.0041, -0.3227, -0.1123]]],\n",
      "\n",
      "\n",
      "        [[[-0.1712,  0.1013, -0.1530],\n",
      "          [-0.1357,  0.0605,  0.2320],\n",
      "          [ 0.1906, -0.1265, -0.1705]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1012,  0.2104, -0.1162],\n",
      "          [-0.2529, -0.2708,  0.1293],\n",
      "          [-0.2487,  0.3000, -0.0206]]],\n",
      "\n",
      "\n",
      "        [[[-0.1447,  0.1170, -0.1302],\n",
      "          [ 0.1352, -0.2867,  0.0238],\n",
      "          [-0.3095, -0.2621,  0.3269]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0315,  0.3282, -0.0881],\n",
      "          [ 0.2434,  0.2364, -0.0231],\n",
      "          [-0.2862,  0.2566,  0.0872]]],\n",
      "\n",
      "\n",
      "        [[[-0.2009,  0.2604, -0.1284],\n",
      "          [ 0.0187, -0.0677,  0.1492],\n",
      "          [ 0.2236, -0.3277, -0.3096]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0841, -0.2351, -0.0785],\n",
      "          [-0.2795,  0.2185, -0.2085],\n",
      "          [-0.0208,  0.2042, -0.1976]]],\n",
      "\n",
      "\n",
      "        [[[-0.2433,  0.0393, -0.3077],\n",
      "          [ 0.0915,  0.2476, -0.2320],\n",
      "          [ 0.1692, -0.1156,  0.0431]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1677, -0.2768,  0.1198],\n",
      "          [ 0.2084,  0.0442,  0.2291],\n",
      "          [ 0.0641, -0.1714, -0.3286]]],\n",
      "\n",
      "\n",
      "        [[[-0.0490,  0.2320,  0.1082],\n",
      "          [ 0.2022,  0.2670, -0.2475],\n",
      "          [-0.3200,  0.1956,  0.2896]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1108, -0.1711, -0.0979],\n",
      "          [-0.0192, -0.2654,  0.2637],\n",
      "          [ 0.3247, -0.2378, -0.0656]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2864, -0.0915,  0.2547],\n",
      "          [ 0.2783, -0.3111,  0.1118],\n",
      "          [ 0.0228, -0.0222, -0.3321]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3100,  0.1299, -0.0720],\n",
      "          [ 0.2608, -0.0694,  0.0549],\n",
      "          [ 0.2899,  0.2128,  0.2787]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3285, -0.1194, -0.2706],\n",
      "          [ 0.0982, -0.2719, -0.1952],\n",
      "          [ 0.1751, -0.0550, -0.0008]]],\n",
      "\n",
      "\n",
      "        [[[-0.1809, -0.2986,  0.0860],\n",
      "          [ 0.0361, -0.1227,  0.1633],\n",
      "          [ 0.0603,  0.1021, -0.0319]]],\n",
      "\n",
      "\n",
      "        [[[-0.2495,  0.2431,  0.1746],\n",
      "          [-0.0056,  0.1415, -0.2243],\n",
      "          [-0.2033, -0.2023,  0.1630]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2971,  0.1107,  0.1575],\n",
      "          [ 0.2763, -0.0971,  0.1919],\n",
      "          [-0.2912,  0.1520,  0.1421]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3323, -0.1422, -0.1641],\n",
      "          [-0.2245,  0.0844, -0.2487],\n",
      "          [-0.0902, -0.3318, -0.2226]]],\n",
      "\n",
      "\n",
      "        [[[-0.2693, -0.2312, -0.0870],\n",
      "          [-0.2348,  0.0190,  0.0972],\n",
      "          [-0.1662,  0.0375, -0.1679]]],\n",
      "\n",
      "\n",
      "        [[[-0.2501, -0.0954, -0.1364],\n",
      "          [-0.1978,  0.2410, -0.2521],\n",
      "          [-0.0683,  0.1310,  0.0176]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2702, -0.1796, -0.1140],\n",
      "          [ 0.1399, -0.0543,  0.1851],\n",
      "          [-0.2816, -0.1567, -0.1362]]],\n",
      "\n",
      "\n",
      "        [[[-0.2673, -0.1209,  0.3102],\n",
      "          [ 0.1569,  0.1417,  0.0479],\n",
      "          [-0.0171, -0.1893, -0.0057]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2394, -0.1831,  0.2349],\n",
      "          [-0.0810, -0.1706, -0.0951],\n",
      "          [-0.1552,  0.0176,  0.0762]]],\n",
      "\n",
      "\n",
      "        [[[-0.3071, -0.0878,  0.2769],\n",
      "          [ 0.2930,  0.1526,  0.0901],\n",
      "          [ 0.2371,  0.2286, -0.2248]]],\n",
      "\n",
      "\n",
      "        [[[-0.1413,  0.2290,  0.1929],\n",
      "          [ 0.0074, -0.2462, -0.3044],\n",
      "          [-0.3174,  0.2499, -0.1951]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1455,  0.3277,  0.1196],\n",
      "          [ 0.3203,  0.1803,  0.1826],\n",
      "          [-0.2530,  0.0325, -0.0933]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2712, -0.1067, -0.1069],\n",
      "          [ 0.1950, -0.0674,  0.2492],\n",
      "          [-0.0844,  0.0822,  0.3206]]],\n",
      "\n",
      "\n",
      "        [[[-0.0068,  0.1922, -0.2653],\n",
      "          [ 0.0422, -0.0944, -0.0841],\n",
      "          [ 0.2739, -0.0190, -0.1171]]]])\n",
      "tensor([-0.1420,  0.3144,  0.2419,  0.3070, -0.1203,  0.2560, -0.3271, -0.0807,\n",
      "        -0.1339, -0.1525, -0.0148, -0.2874,  0.3138,  0.2528,  0.1678, -0.0174,\n",
      "        -0.2264, -0.2955,  0.0518, -0.2241,  0.0416, -0.2992, -0.2148, -0.1406,\n",
      "         0.2834,  0.0407,  0.2451,  0.0551,  0.1716,  0.2241,  0.1168,  0.3023,\n",
      "         0.2435, -0.2763,  0.3276,  0.3235, -0.1823, -0.2119,  0.2904, -0.2926,\n",
      "         0.0654,  0.2080,  0.0285,  0.0948, -0.1725, -0.2047, -0.1987,  0.1520,\n",
      "        -0.0266, -0.2756,  0.2075,  0.0850, -0.2503, -0.1484, -0.0023, -0.2380,\n",
      "        -0.2478,  0.0412, -0.3069,  0.2996, -0.2065,  0.1925, -0.1029,  0.0751])\n",
      "tensor([[-8.8536e-03,  4.2036e-03,  2.0814e-02, -3.5566e-03, -7.4969e-03,\n",
      "         -3.8069e-02, -1.8093e-02, -1.9518e-02,  3.2573e-02,  3.3604e-02,\n",
      "         -3.1533e-02,  4.1480e-02,  1.4040e-02,  2.7852e-02,  1.7995e-02,\n",
      "          1.6713e-03, -7.2303e-04, -3.7183e-02, -2.7249e-02, -2.5915e-02,\n",
      "         -4.7746e-04,  1.3236e-02,  1.7089e-02, -3.4090e-02,  1.8836e-02,\n",
      "          4.0174e-02, -2.4127e-03,  9.5949e-03,  3.6199e-02, -1.2928e-02,\n",
      "          1.0635e-02, -2.2103e-02,  9.8593e-03, -2.3585e-02,  2.2966e-02,\n",
      "         -2.8961e-02, -3.6228e-02, -1.5017e-02,  3.9050e-02,  3.8077e-02,\n",
      "          1.6746e-02,  3.9130e-02, -2.5454e-02,  3.5845e-02,  5.0397e-03,\n",
      "          4.1146e-02,  3.5322e-02, -2.9462e-02, -8.1369e-03, -3.5677e-02,\n",
      "          1.6708e-02, -7.5337e-03,  3.4322e-02,  2.9809e-02, -8.1556e-03,\n",
      "          2.1260e-02,  1.8386e-02,  2.2451e-02, -1.0688e-02,  7.6818e-03,\n",
      "         -2.3196e-02, -3.7708e-02, -9.5695e-04, -6.8460e-03, -9.2763e-03,\n",
      "         -4.7991e-03,  1.0589e-02, -3.7514e-02, -3.2518e-02, -3.6847e-02,\n",
      "          1.5357e-02,  4.0916e-02,  1.0943e-02, -1.9536e-02,  6.0042e-03,\n",
      "          2.2583e-02, -6.3750e-03,  1.3649e-02,  3.5766e-02,  1.9272e-03,\n",
      "          1.1983e-02, -4.3374e-03, -9.1785e-03, -3.8860e-02,  2.4593e-02,\n",
      "          3.3614e-02, -3.6292e-02,  3.2965e-02, -3.5967e-02, -2.8493e-02,\n",
      "         -1.9090e-02, -1.2009e-02,  2.1289e-02,  3.2311e-02,  3.5905e-02,\n",
      "          2.4919e-02,  3.4874e-02, -2.9871e-02, -2.3195e-03,  3.7525e-02,\n",
      "          1.4067e-02, -2.1996e-03,  1.9649e-03,  9.5780e-03,  2.6857e-02,\n",
      "         -2.9140e-02, -1.1201e-03, -1.7951e-02,  2.2459e-02, -1.9104e-02,\n",
      "          3.5866e-02,  3.2604e-03,  3.0705e-02, -2.6345e-02,  2.5574e-02,\n",
      "         -4.5182e-03,  1.9485e-02,  5.7812e-03, -4.1141e-02, -7.5724e-03,\n",
      "         -2.3760e-03,  3.0072e-02,  1.6459e-02,  1.5855e-02, -4.0084e-03,\n",
      "          3.0951e-02,  3.0188e-02, -3.3785e-02, -3.0727e-02,  3.1468e-02,\n",
      "         -3.9863e-02,  3.2071e-03,  1.1172e-02, -1.5105e-02, -3.2852e-03,\n",
      "          1.8160e-02, -3.2970e-02,  7.4884e-03,  1.1116e-02,  1.4934e-02,\n",
      "         -1.9500e-02,  1.0893e-03,  1.9468e-02, -2.8655e-02, -1.2390e-02,\n",
      "          1.8981e-02, -1.3887e-02, -3.4714e-02,  2.4533e-02,  3.3207e-03,\n",
      "         -2.0409e-02,  1.1948e-02, -1.2766e-02,  1.4477e-02,  3.8176e-02,\n",
      "          4.4310e-03, -2.2708e-02,  4.1470e-02, -1.9819e-02,  2.2214e-02,\n",
      "         -3.7174e-02, -6.4631e-03, -1.6016e-02,  1.5991e-02,  1.4309e-03,\n",
      "          1.4768e-03, -3.8140e-03,  9.7139e-03,  2.9822e-02,  1.6068e-02,\n",
      "         -1.4199e-02,  1.4028e-02,  9.2101e-03, -1.0131e-02,  3.3838e-02,\n",
      "          4.1147e-02, -3.2650e-02,  3.2710e-02, -2.9669e-02,  4.1593e-02,\n",
      "          2.8896e-02, -1.3430e-02, -5.4843e-03, -2.4275e-02, -9.7473e-03,\n",
      "          1.4810e-02,  3.6352e-03, -3.4947e-02,  2.9983e-02, -1.5307e-02,\n",
      "          4.0861e-02,  4.0212e-02, -3.9724e-02,  7.9535e-04, -1.2763e-03,\n",
      "          2.2684e-02, -2.1672e-02, -3.6855e-03,  7.5844e-03, -1.8472e-02,\n",
      "         -3.6098e-02,  2.6255e-02, -7.9911e-03, -8.7715e-03,  2.6579e-02,\n",
      "          1.7244e-02,  2.9291e-02,  2.0141e-02, -2.9722e-03,  1.2720e-03,\n",
      "          3.9695e-02, -4.0821e-02,  4.0204e-02, -9.3239e-03,  3.7775e-02,\n",
      "         -3.2018e-02,  3.0743e-02,  7.7691e-03,  1.3530e-02,  8.4594e-03,\n",
      "          5.8367e-03,  2.0793e-02, -1.7356e-02, -1.3800e-02, -2.9249e-02,\n",
      "          6.5089e-03,  4.1168e-02, -3.6133e-03, -1.8108e-02, -5.5678e-03,\n",
      "          1.3288e-02,  1.1645e-03, -1.6628e-03, -2.8300e-02,  2.6365e-02,\n",
      "         -1.2261e-02, -2.5157e-03, -8.2273e-03, -3.6180e-02,  1.9312e-02,\n",
      "         -2.5918e-02, -4.0277e-02, -3.4619e-02, -9.1386e-03,  1.3238e-02,\n",
      "         -1.7905e-02, -4.0241e-02,  1.1953e-02,  9.2980e-03,  4.0422e-03,\n",
      "          1.9413e-02,  3.4120e-02, -1.6336e-02,  1.6992e-02,  5.5545e-03,\n",
      "          1.2960e-02, -1.8880e-02,  2.2707e-02,  2.2120e-02, -1.1542e-02,\n",
      "          4.1546e-02, -1.7814e-02, -3.3929e-02,  4.1361e-02, -6.0678e-05,\n",
      "          2.9958e-03, -2.9362e-02,  3.3706e-02,  2.7988e-03,  3.6700e-02,\n",
      "          1.3082e-02, -3.7436e-02, -3.4164e-02, -2.3923e-02,  3.4672e-02,\n",
      "         -4.1551e-02, -1.0464e-02,  2.3366e-02,  1.0668e-02,  3.1741e-02,\n",
      "          2.0312e-02,  4.0194e-02,  1.3526e-02, -7.2392e-03,  7.2508e-03,\n",
      "          3.4923e-02,  1.5413e-02,  2.6255e-02, -2.3731e-02,  1.0034e-02,\n",
      "         -6.8975e-03,  2.7029e-02, -1.7990e-02,  3.1595e-02, -3.8535e-02,\n",
      "          2.3655e-03,  4.0038e-02, -2.5217e-02, -2.2112e-02,  3.6877e-03,\n",
      "         -1.6250e-02, -3.7154e-02,  4.6095e-03,  1.9474e-02, -2.0510e-02,\n",
      "          2.9613e-02, -2.9345e-02, -3.3382e-02,  2.9765e-02, -1.9941e-02,\n",
      "         -1.3624e-02, -3.7620e-02,  3.6309e-02, -1.3380e-02,  7.4253e-03,\n",
      "         -1.4200e-02,  3.5692e-02,  6.4081e-03,  3.9555e-02,  3.0088e-02,\n",
      "         -1.3142e-02, -3.3473e-02,  3.5823e-02,  5.6022e-03, -1.0739e-02,\n",
      "         -3.8778e-02,  1.3199e-03,  3.0969e-02,  2.6518e-02,  3.2826e-02,\n",
      "          5.5678e-03, -2.1158e-03, -1.2845e-02, -1.9481e-02, -3.8351e-02,\n",
      "          2.7148e-02,  2.3070e-02,  1.8794e-02, -9.3019e-04,  1.4982e-02,\n",
      "         -1.6973e-02, -1.5663e-02,  1.0180e-02, -5.5955e-04,  4.0520e-02,\n",
      "          2.0731e-02,  1.9524e-02, -6.8802e-03,  2.7007e-02, -3.5634e-02,\n",
      "          3.5312e-02,  1.6649e-02, -3.9986e-02,  3.7143e-02, -2.8062e-02,\n",
      "          1.0945e-03, -3.9447e-02, -1.1138e-02, -3.6075e-02,  3.2876e-02,\n",
      "         -2.2084e-02, -3.5690e-02, -4.9930e-03, -1.3423e-02, -3.7235e-02,\n",
      "         -2.7474e-04,  2.1150e-02,  2.1760e-02, -4.0259e-02,  1.0922e-02,\n",
      "         -3.5608e-02,  7.0785e-03,  1.4379e-02,  3.1119e-02,  1.7426e-02,\n",
      "          1.0172e-02,  1.2747e-02, -1.7365e-02,  2.8672e-02, -1.8246e-02,\n",
      "         -6.5192e-03, -6.2190e-03,  3.3371e-02,  2.2433e-02,  1.0551e-02,\n",
      "         -1.9784e-02, -7.9613e-04, -7.4905e-03,  9.7012e-03,  7.0015e-03,\n",
      "         -2.2060e-02,  2.4464e-02,  2.7454e-03,  3.4479e-02,  2.3423e-02,\n",
      "          2.1828e-02,  3.3047e-02, -3.1529e-02, -1.7774e-02, -2.3872e-02,\n",
      "          1.8234e-02,  5.0604e-03,  2.3674e-02,  3.3761e-02, -3.6147e-02,\n",
      "         -2.3324e-02, -3.8647e-02,  6.5361e-03, -1.4230e-02,  2.0509e-02,\n",
      "          3.7771e-02, -1.7665e-03, -2.2795e-02,  3.5267e-02, -1.3954e-02,\n",
      "          1.9160e-02, -3.9585e-02, -3.3561e-02,  1.2197e-02, -3.9961e-02,\n",
      "         -2.2280e-02,  3.8062e-02,  4.0840e-02,  3.7936e-02, -2.8407e-02,\n",
      "          7.6741e-03, -2.0594e-02,  9.8882e-03,  2.5981e-02,  1.2411e-03,\n",
      "         -1.1686e-02, -1.8418e-02, -9.2529e-03, -3.8351e-02,  1.4815e-02,\n",
      "          4.0452e-02, -1.8547e-02,  5.3798e-03, -2.7480e-02, -3.2605e-02,\n",
      "         -2.6378e-02,  3.4410e-02, -2.4813e-02, -1.0405e-02, -9.6533e-03,\n",
      "         -2.7184e-02,  2.2715e-02, -1.1648e-02,  8.5705e-04, -3.0361e-02,\n",
      "          1.1729e-02,  1.3676e-02, -1.9063e-02,  1.4899e-02, -2.4207e-02,\n",
      "          1.6793e-02, -2.6155e-02, -3.2425e-02, -1.2043e-02,  3.7791e-02,\n",
      "         -2.0763e-02, -7.0323e-03,  1.2140e-03,  2.7998e-02,  4.1503e-02,\n",
      "         -1.5010e-02, -2.2318e-02, -2.9156e-02,  2.5291e-02, -2.2347e-02,\n",
      "         -1.4265e-02,  4.5766e-03, -2.0535e-02, -2.6849e-02,  4.4958e-03,\n",
      "          9.0093e-03, -7.9010e-03,  1.9336e-02,  3.9271e-02,  3.7228e-02,\n",
      "          4.0791e-02, -3.6734e-02,  2.0868e-02,  1.8707e-02, -6.1394e-03,\n",
      "         -1.1981e-02, -3.3129e-02, -2.8448e-02,  9.3273e-03,  2.7680e-02,\n",
      "          3.1102e-02,  3.8320e-02,  1.8520e-03,  1.7840e-02, -2.0284e-02,\n",
      "         -3.8051e-02,  1.5488e-02,  2.1639e-03, -4.0872e-02,  3.8487e-02,\n",
      "         -2.2909e-02, -4.1066e-02,  5.9852e-03,  1.1951e-02, -1.1322e-02,\n",
      "          2.8917e-02,  6.9173e-03, -1.7566e-02,  1.6506e-02, -3.0382e-02,\n",
      "         -3.1930e-02,  3.8588e-02, -1.8332e-02,  3.9807e-02,  2.3401e-02,\n",
      "         -1.0192e-02, -6.8769e-03,  2.3746e-02,  8.2593e-03, -1.2216e-02,\n",
      "          1.8924e-02,  4.0719e-02, -3.9730e-02,  1.1194e-02, -2.0578e-02,\n",
      "         -2.7745e-02, -2.0808e-02,  1.6878e-03, -3.7640e-02, -3.2295e-02,\n",
      "         -1.2188e-03,  3.7113e-02,  1.6573e-02,  1.8734e-02, -2.5698e-02,\n",
      "          1.0977e-02,  1.0340e-02,  2.9369e-02, -1.0908e-02, -1.2319e-02,\n",
      "          1.5126e-02, -4.0901e-02, -1.9941e-02, -3.6736e-03,  2.2455e-02,\n",
      "          3.1818e-02,  1.4382e-02,  3.3932e-02, -3.5222e-02,  3.2068e-02,\n",
      "         -3.9172e-02,  9.1360e-03, -1.5211e-02, -4.1562e-02, -3.9295e-03,\n",
      "          4.1344e-02, -3.3544e-02, -3.3794e-02, -2.0172e-03,  3.4845e-02,\n",
      "         -3.4176e-02, -2.0698e-02, -9.4300e-03, -2.4725e-02, -6.0047e-03,\n",
      "          7.6370e-03, -2.8580e-02,  3.7129e-02,  2.3530e-02, -2.9290e-02,\n",
      "          1.4957e-02,  2.9259e-03,  1.6622e-02, -9.7471e-03,  2.4390e-02,\n",
      "          2.4201e-03]])\n",
      "tensor([-0.0257])\n",
      "Epoch [0/1000], Loss: 82.00382232666016\n",
      "Epoch [100/1000], Loss: 4.387120246887207\n",
      "Epoch [200/1000], Loss: 0.07716096192598343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Desktop\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/1000], Loss: 0.0013570773880928755\n",
      "Epoch [400/1000], Loss: 2.386980486335233e-05\n",
      "Epoch [500/1000], Loss: 4.193143468000926e-07\n",
      "Epoch [600/1000], Loss: 7.531525625381619e-09\n",
      "Epoch [700/1000], Loss: 5.238689482212067e-10\n",
      "Epoch [800/1000], Loss: 5.238689482212067e-10\n",
      "Epoch [900/1000], Loss: 5.238689482212067e-10\n",
      "Predicted output: 8.999977111816406\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Binarized Conv2D Layer\n",
    "class BinarizedConv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(BinarizedConv2D, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    " \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Binarize the weights\n",
    "        binarized_weights = torch.sign(self.conv.weight)\n",
    "        return F.conv2d(x, binarized_weights, self.conv.bias, self.conv.stride, self.conv.padding)\n",
    "\n",
    "# Simple Network\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.binarized_conv = BinarizedConv2D(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(64 * 3 * 3, 1)  # Flattened 3x3 output to a single scalar\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.binarized_conv(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the network\n",
    "model = SimpleNet()\n",
    "for param in model.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.data)\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Prepare the data\n",
    "input_tensor = torch.ones(1, 1, 3, 3)  # Batch size 1, 1 channel, 3x3 image\n",
    "target = torch.tensor([9.0])  # Correct answer is 9\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model(input_tensor)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Test the network\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "    print(f'Predicted output: {output.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BNNConv2d' from 'custom_bnn' (d:\\study\\S3\\Project thesis\\Work\\custom_bnn.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustom_bnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BNNConv2d\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSimpleNet2\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule): \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'BNNConv2d' from 'custom_bnn' (d:\\study\\S3\\Project thesis\\Work\\custom_bnn.py)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from custom_bnn import BNNConv2d\n",
    "class SimpleNet2(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(SimpleNet2, self).__init__()\n",
    "        self.binarized_conv = BNNConv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(64 * 3 * 3, 1)  # Flattened 3x3 output to a single scalar\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.binarized_conv(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the network\n",
    "model2 = SimpleNet2()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.0001)\n",
    "\n",
    "# Prepare the data\n",
    "input_tensor = torch.ones(1, 1, 3, 3)  # Batch size 1, 1 channel, 3x3 image\n",
    "target = torch.tensor([-18.0])  # Correct answer is 9\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    model2.train()\n",
    "    \n",
    "    # Zero gradients\n",
    "    optimizer2.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output = model2(input_tensor)\n",
    "    \n",
    "    # Compute the loss\n",
    "    loss2 = criterion(output, target)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss2.backward()\n",
    "    optimizer2.step()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss: {loss2.item()}')\n",
    "\n",
    "# Test the network\n",
    "model2.eval()\n",
    "with torch.no_grad():\n",
    "    output = model2(input_tensor)\n",
    "    print(f'Predicted output: {output.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model2.parameters():\n",
    "    if torch.isnan(param).any():\n",
    "        print(\"NaN detected in parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-3.0531e-04,  5.9963e-04, -3.1483e-04],\n",
      "          [-4.0678e-04, -4.7712e-04, -2.7545e-04],\n",
      "          [-1.8555e-04,  3.1659e-04, -4.6537e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 7.3230e-04, -2.4850e-05, -5.2456e-04],\n",
      "          [-4.2127e-04, -9.8822e-05, -1.3536e-04],\n",
      "          [-9.4681e-04,  7.1551e-04,  3.4888e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.4143e-05,  6.8131e-04,  9.3344e-04],\n",
      "          [ 1.3991e-03,  1.6811e-03,  1.1179e-03],\n",
      "          [-7.9395e-05,  5.1198e-04,  7.9454e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0490e-04, -8.2551e-04,  4.1526e-04],\n",
      "          [-6.1821e-04,  1.2758e-03,  5.8175e-04],\n",
      "          [ 1.1742e-04, -4.0906e-04,  9.7777e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0363e-04, -1.3289e-04, -5.9140e-04],\n",
      "          [ 1.0974e-03,  1.2387e-03,  1.7357e-04],\n",
      "          [ 9.8778e-04,  9.4199e-04,  1.3228e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2046e-03,  9.8128e-04, -3.6269e-04],\n",
      "          [ 1.0454e-03,  9.7377e-04, -5.0830e-04],\n",
      "          [ 1.0988e-03,  4.7435e-04, -2.8166e-04]]],\n",
      "\n",
      "\n",
      "        [[[-9.7225e-04, -3.8115e-05, -3.1524e-05],\n",
      "          [-7.6199e-04, -8.5860e-04, -7.8835e-04],\n",
      "          [-8.7072e-04, -7.3839e-04,  8.3253e-04]]],\n",
      "\n",
      "\n",
      "        [[[-4.6743e-04,  1.5388e-06,  5.4687e-04],\n",
      "          [-7.8024e-04, -7.7821e-04, -9.3301e-04],\n",
      "          [ 5.6598e-04, -7.6512e-04, -4.0262e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 7.6204e-04,  7.5914e-04,  5.7296e-04],\n",
      "          [ 5.4328e-04, -7.6086e-04, -2.0265e-04],\n",
      "          [-3.6211e-04,  4.2084e-04,  3.3852e-04]]],\n",
      "\n",
      "\n",
      "        [[[-6.4654e-04, -8.4336e-04,  3.1433e-05],\n",
      "          [ 4.0071e-04,  1.9020e-04, -2.2895e-04],\n",
      "          [ 2.0955e-04, -2.2093e-04, -2.3518e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.5913e-05, -3.6377e-04,  7.7139e-04],\n",
      "          [ 3.1949e-04, -7.0355e-04,  3.1924e-04],\n",
      "          [ 4.9378e-04, -5.5430e-04,  3.9705e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 6.8065e-04, -5.6169e-04,  5.8485e-04],\n",
      "          [-7.6146e-04,  9.4175e-04,  1.0232e-03],\n",
      "          [-8.7887e-05,  2.7030e-04,  9.8022e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 5.9190e-04,  8.9126e-04, -4.7775e-04],\n",
      "          [-4.9473e-04, -7.9078e-04, -5.8276e-04],\n",
      "          [-3.5486e-04, -7.2654e-04,  7.7325e-05]]],\n",
      "\n",
      "\n",
      "        [[[-2.5647e-04, -8.0402e-04,  8.6315e-04],\n",
      "          [ 6.6888e-04,  4.1431e-04, -2.0743e-04],\n",
      "          [-1.0899e-03, -2.9403e-04, -3.8744e-04]]],\n",
      "\n",
      "\n",
      "        [[[-2.8738e-04,  1.7929e-04,  6.0249e-04],\n",
      "          [ 5.0379e-04, -4.4920e-04, -6.7789e-04],\n",
      "          [-1.2827e-04, -9.1330e-04,  2.2494e-04]]],\n",
      "\n",
      "\n",
      "        [[[-4.5888e-04,  5.6040e-04, -9.8687e-04],\n",
      "          [ 5.1851e-04,  2.0601e-04, -9.5178e-04],\n",
      "          [-1.0316e-03,  1.6380e-04, -2.5159e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 8.1198e-04,  1.8312e-03,  1.7998e-03],\n",
      "          [ 2.5247e-04,  1.3455e-03,  1.4986e-03],\n",
      "          [ 5.5135e-04, -7.2337e-05,  2.5080e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.2382e-04,  8.1025e-04, -4.3161e-05],\n",
      "          [ 8.2250e-04,  1.6533e-03,  1.9299e-03],\n",
      "          [ 5.1084e-04,  2.5719e-04, -2.6373e-05]]],\n",
      "\n",
      "\n",
      "        [[[-8.4112e-04, -1.3147e-04,  2.2701e-04],\n",
      "          [ 1.1167e-04, -3.5606e-04, -3.9847e-04],\n",
      "          [-8.8338e-04,  5.1599e-04,  9.2542e-04]]],\n",
      "\n",
      "\n",
      "        [[[-4.5552e-04,  6.5923e-04,  5.7832e-04],\n",
      "          [ 9.4312e-04, -6.0420e-04,  6.7519e-04],\n",
      "          [ 1.3225e-03, -1.7844e-04,  1.7879e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 6.5336e-04, -3.4216e-04,  3.7124e-04],\n",
      "          [-9.9166e-04, -9.2764e-04, -4.1138e-04],\n",
      "          [-3.3299e-04, -7.4277e-05,  6.0501e-04]]],\n",
      "\n",
      "\n",
      "        [[[-5.4680e-05, -6.9110e-04,  3.3705e-04],\n",
      "          [-1.1204e-04, -5.6382e-04, -7.9762e-04],\n",
      "          [ 3.6418e-04,  6.3633e-04, -1.3479e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.2880e-04,  1.2648e-03,  1.5420e-03],\n",
      "          [ 9.8688e-04,  1.5055e-03,  2.3096e-03],\n",
      "          [ 2.9875e-04,  2.0562e-03,  1.6770e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2133e-03,  1.3162e-03, -3.4362e-04],\n",
      "          [ 6.8013e-04,  2.1514e-03,  7.5368e-04],\n",
      "          [ 1.1556e-03,  1.7336e-03,  1.0593e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1554e-03,  4.7986e-04,  2.0160e-04],\n",
      "          [ 1.5667e-03,  1.3813e-03,  2.1700e-04],\n",
      "          [ 1.7051e-03, -3.5383e-05,  1.1429e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.9003e-04,  2.7209e-04, -2.2559e-04],\n",
      "          [-2.9181e-04, -2.8827e-04, -7.6690e-04],\n",
      "          [-3.5741e-04, -5.8290e-04,  7.3396e-04]]],\n",
      "\n",
      "\n",
      "        [[[-5.3701e-04,  8.2644e-04,  4.3412e-04],\n",
      "          [-7.2400e-04, -5.3143e-04, -6.6633e-04],\n",
      "          [ 7.2160e-04, -7.2321e-04, -5.6139e-04]]],\n",
      "\n",
      "\n",
      "        [[[-5.5226e-04,  5.7200e-04, -1.3310e-04],\n",
      "          [-5.3065e-04, -9.0222e-04, -1.8810e-04],\n",
      "          [-7.2629e-04,  4.6382e-04,  8.5983e-06]]],\n",
      "\n",
      "\n",
      "        [[[-5.5780e-04,  3.5275e-04, -1.3541e-04],\n",
      "          [-9.4312e-05,  8.6615e-04,  6.8338e-04],\n",
      "          [-7.0851e-04, -8.9768e-04, -3.5977e-04]]],\n",
      "\n",
      "\n",
      "        [[[-7.0160e-04, -2.3724e-04, -5.0685e-04],\n",
      "          [ 8.5944e-04, -9.9497e-04, -1.2547e-04],\n",
      "          [ 1.2690e-05, -8.8820e-04, -1.6463e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 6.3980e-04,  5.8182e-04,  8.0307e-04],\n",
      "          [ 4.2705e-04,  1.9267e-03,  2.1828e-03],\n",
      "          [ 1.3813e-03,  2.2985e-03,  1.9300e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4965e-03,  1.2535e-03, -2.9536e-04],\n",
      "          [ 1.2486e-03,  1.6132e-03,  8.4988e-04],\n",
      "          [-2.9338e-04, -5.1593e-04, -6.7561e-05]]],\n",
      "\n",
      "\n",
      "        [[[-7.1719e-04,  7.0337e-05, -8.9357e-04],\n",
      "          [-1.3058e-03,  7.1539e-04,  1.5350e-05],\n",
      "          [-1.1468e-03,  5.3210e-04,  7.7282e-06]]],\n",
      "\n",
      "\n",
      "        [[[-1.2937e-04,  1.3123e-03,  1.2175e-03],\n",
      "          [ 1.0773e-03,  9.3695e-04, -5.9372e-06],\n",
      "          [ 7.4382e-04, -3.4600e-04, -2.0321e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 7.1499e-04, -8.0076e-04,  2.2683e-05],\n",
      "          [-1.3549e-04,  4.7445e-04, -8.8225e-04],\n",
      "          [-3.7626e-04, -8.8267e-04, -2.2204e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.0649e-04,  1.6888e-03,  9.5801e-04],\n",
      "          [-1.3220e-06,  1.6751e-03,  8.7349e-04],\n",
      "          [ 4.1406e-04, -1.1486e-04, -5.8842e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 6.8899e-04,  8.2060e-04,  6.4126e-04],\n",
      "          [ 9.8712e-04,  1.0573e-03,  1.9170e-04],\n",
      "          [-4.6298e-04, -1.9114e-04, -9.1264e-04]]],\n",
      "\n",
      "\n",
      "        [[[-7.0337e-05, -1.8387e-04, -2.0292e-04],\n",
      "          [-5.4052e-04, -7.4712e-04, -7.7314e-04],\n",
      "          [-7.5299e-04,  8.3267e-04, -5.8951e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 9.5344e-05,  4.0929e-04, -7.1798e-04],\n",
      "          [ 4.8929e-04,  9.2635e-04, -4.6854e-04],\n",
      "          [-6.8039e-04,  1.8628e-06, -8.3332e-04]]],\n",
      "\n",
      "\n",
      "        [[[-8.6259e-04,  5.5135e-04,  2.7077e-04],\n",
      "          [-9.8299e-05, -9.0870e-04, -4.2848e-04],\n",
      "          [-4.4349e-04,  1.8974e-04,  2.1662e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 7.4926e-04,  4.4497e-04,  5.9301e-04],\n",
      "          [ 5.3066e-04, -1.5172e-04, -2.5866e-04],\n",
      "          [ 2.8000e-04, -1.3653e-04, -4.3382e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.0886e-05,  1.8512e-04, -3.8285e-04],\n",
      "          [-9.2927e-04,  2.3060e-04,  2.7945e-04],\n",
      "          [-3.8584e-04, -5.4683e-04, -1.8396e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.4659e-04, -5.5046e-04,  2.9200e-04],\n",
      "          [ 8.1148e-04, -4.7125e-04, -1.7978e-04],\n",
      "          [-9.4909e-04, -6.9384e-04, -6.6030e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 7.9659e-05, -7.9745e-05,  5.4652e-04],\n",
      "          [ 5.5665e-04,  1.3518e-04,  4.6015e-04],\n",
      "          [ 2.8179e-04, -4.1428e-04, -6.4730e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 4.7069e-04,  5.7474e-04, -8.1705e-04],\n",
      "          [-6.9436e-04, -6.0806e-04, -7.6442e-04],\n",
      "          [-6.7977e-04, -7.6398e-05, -1.6680e-04]]],\n",
      "\n",
      "\n",
      "        [[[-6.4498e-04, -6.3328e-04, -2.5684e-04],\n",
      "          [-5.4330e-06, -2.8522e-04,  6.8202e-04],\n",
      "          [ 7.0406e-04, -8.9017e-04,  9.1916e-04]]],\n",
      "\n",
      "\n",
      "        [[[-1.5189e-04, -1.8200e-04,  7.4986e-04],\n",
      "          [-7.1492e-04, -7.8034e-04, -9.2060e-04],\n",
      "          [-1.2536e-04, -1.2646e-04,  1.3058e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 2.6261e-05, -8.9889e-04,  3.7807e-04],\n",
      "          [-3.1899e-05,  1.9927e-04, -3.4540e-04],\n",
      "          [-4.3037e-04,  5.7978e-04, -4.3186e-04]]],\n",
      "\n",
      "\n",
      "        [[[-7.4865e-04, -4.0027e-04, -1.4138e-04],\n",
      "          [ 1.2250e-03,  1.6740e-03,  1.4639e-03],\n",
      "          [ 9.1842e-05,  2.2805e-03,  1.2195e-03]]],\n",
      "\n",
      "\n",
      "        [[[-9.5801e-04,  2.0883e-04,  7.4806e-04],\n",
      "          [-8.8638e-04, -1.6768e-04, -8.0270e-04],\n",
      "          [ 1.4188e-05, -8.8204e-05, -2.8755e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5865e-05, -6.4714e-04, -9.0531e-05],\n",
      "          [-2.4378e-04,  6.9989e-04,  9.8565e-04],\n",
      "          [-1.9989e-04,  5.5459e-04, -4.4179e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 9.0090e-04, -1.6987e-04, -8.9079e-04],\n",
      "          [-3.7511e-04, -1.6501e-04,  1.1973e-04],\n",
      "          [-6.2293e-04, -6.9846e-04, -3.5382e-04]]],\n",
      "\n",
      "\n",
      "        [[[-7.5455e-04,  6.6457e-04, -7.9311e-04],\n",
      "          [-6.5130e-04,  5.9443e-04,  3.6334e-04],\n",
      "          [-8.6679e-04, -4.3335e-04, -2.8827e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 6.0966e-04, -2.1574e-04,  3.2268e-04],\n",
      "          [-4.7572e-04,  2.8189e-04, -8.1187e-04],\n",
      "          [ 1.1425e-04, -1.0927e-04, -7.5384e-04]]],\n",
      "\n",
      "\n",
      "        [[[-3.0678e-04,  7.8229e-04,  9.7384e-04],\n",
      "          [-6.3453e-04, -3.3559e-04, -4.4200e-05],\n",
      "          [-3.6338e-04, -6.0619e-05,  5.9682e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2702e-03,  5.4646e-04,  1.0464e-03],\n",
      "          [ 8.3172e-04,  1.3958e-03,  5.1058e-04],\n",
      "          [-8.1693e-04, -8.8117e-04, -1.7924e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 1.2030e-03,  1.4742e-03,  1.5979e-03],\n",
      "          [ 2.2794e-03,  3.1222e-03,  2.8882e-03],\n",
      "          [ 1.5285e-03,  8.1891e-04,  1.6648e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.7884e-04, -5.0654e-04,  4.6347e-04],\n",
      "          [-5.9452e-04, -3.9951e-04, -8.2848e-04],\n",
      "          [-8.7970e-04,  6.2875e-04, -8.6444e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1629e-04,  8.6843e-04,  4.3441e-04],\n",
      "          [ 1.8367e-03,  1.1332e-03,  8.0735e-04],\n",
      "          [ 1.0703e-03,  1.4917e-03,  8.1963e-04]]],\n",
      "\n",
      "\n",
      "        [[[-5.9661e-04, -5.0128e-04,  1.1321e-03],\n",
      "          [ 1.2403e-03,  1.6280e-04,  5.3299e-04],\n",
      "          [ 1.3445e-03,  7.5957e-04, -1.6545e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 7.5827e-04, -1.9609e-04,  9.3743e-04],\n",
      "          [-3.3958e-04,  3.0998e-04, -2.2705e-04],\n",
      "          [-9.0769e-04, -5.6287e-04, -2.6616e-04]]],\n",
      "\n",
      "\n",
      "        [[[-7.8667e-04, -2.0095e-04, -1.7363e-04],\n",
      "          [ 1.5494e-03,  3.8505e-04,  8.3893e-04],\n",
      "          [ 7.3631e-04,  1.1270e-03, -2.0542e-04]]],\n",
      "\n",
      "\n",
      "        [[[ 3.4980e-04,  8.9164e-05,  4.5932e-04],\n",
      "          [ 8.6854e-04,  3.9698e-04, -1.0387e-03],\n",
      "          [-3.6847e-04,  2.7573e-05,  1.2659e-03]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0102e-04,  1.8636e-04,  4.3583e-04],\n",
      "          [-2.9379e-04, -2.0710e-05, -4.2937e-04],\n",
      "          [-6.5465e-05, -2.3210e-04, -4.4041e-04]]]])\n",
      "tensor([[-1., -1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "        [-1., -1., -1., -1., -1., -1., -1.,  1., -1.],\n",
      "        [-1., -1., -1., -1., -1.,  1.,  1., -1.,  1.],\n",
      "        [-1., -1., -1.,  1., -1., -1.,  1., -1., -1.],\n",
      "        [-1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.],\n",
      "        [-1., -1., -1.,  1.,  1.,  1., -1.,  1.,  1.],\n",
      "        [-1., -1.,  1., -1., -1., -1., -1., -1.,  1.],\n",
      "        [-1., -1.,  1., -1., -1., -1.,  1.,  1., -1.],\n",
      "        [-1., -1.,  1.,  1., -1., -1., -1.,  1.,  1.],\n",
      "        [-1., -1.,  1.,  1., -1.,  1.,  1., -1.,  1.],\n",
      "        [-1., -1.,  1.,  1., -1.,  1.,  1.,  1., -1.],\n",
      "        [-1., -1.,  1.,  1.,  1., -1., -1.,  1., -1.],\n",
      "        [-1., -1.,  1.,  1.,  1., -1.,  1., -1., -1.],\n",
      "        [-1.,  1., -1., -1., -1., -1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1., -1., -1., -1., -1.,  1.,  1.],\n",
      "        [-1.,  1., -1., -1.,  1.,  1., -1., -1., -1.],\n",
      "        [-1.,  1., -1., -1.,  1.,  1., -1.,  1.,  1.],\n",
      "        [-1.,  1., -1.,  1., -1.,  1.,  1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1.,  1., -1., -1.,  1., -1.],\n",
      "        [-1.,  1., -1.,  1.,  1.,  1.,  1.,  1., -1.],\n",
      "        [-1.,  1.,  1., -1., -1., -1., -1., -1.,  1.],\n",
      "        [-1.,  1.,  1., -1., -1., -1., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  1., -1., -1., -1.,  1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  1., -1., -1., -1., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.,  1., -1.,  1.,  1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  1.,  1., -1., -1., -1.,  1.],\n",
      "        [-1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.],\n",
      "        [-1.,  1.,  1.,  1.,  1.,  1., -1.,  1.,  1.],\n",
      "        [ 1., -1., -1., -1., -1., -1., -1.,  1.,  1.],\n",
      "        [ 1., -1., -1., -1., -1.,  1., -1., -1., -1.],\n",
      "        [ 1., -1., -1., -1.,  1.,  1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1., -1.,  1., -1., -1., -1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1., -1., -1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1., -1.,  1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.,  1., -1., -1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  1., -1.,  1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.],\n",
      "        [ 1., -1.,  1.,  1., -1., -1., -1., -1., -1.],\n",
      "        [ 1., -1.,  1.,  1.,  1.,  1.,  1., -1., -1.],\n",
      "        [ 1.,  1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "        [ 1.,  1., -1., -1., -1., -1., -1., -1.,  1.],\n",
      "        [ 1.,  1., -1., -1.,  1., -1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1., -1., -1.,  1.,  1., -1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  1.,  1., -1.,  1., -1., -1.],\n",
      "        [ 1.,  1., -1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "        [ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.],\n",
      "        [ 1.,  1.,  1., -1.,  1.,  1.,  1., -1., -1.],\n",
      "        [ 1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1., -1., -1.,  1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.,  1., -1.,  1., -1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1., -1., -1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1., -1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1., -1., -1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]])\n",
      "tensor([13, 29, 28, 38,  4, 45,  0, 22, 51, 12,  9, 37, 42, 11, 23, 18, 48, 19,\n",
      "         8, 25, 32,  7, 24, 43, 53, 42, 22, 14, 15,  3, 49, 46, 16, 27, 34, 55,\n",
      "        52,  1, 18, 21, 50, 44, 39, 40, 41,  2,  6, 35,  5, 22, 31, 30, 15, 36,\n",
      "        20, 54, 56, 33, 57, 10, 34, 17, 26, 47], dtype=torch.uint8)\n",
      "tensor([[ 3.0736e-02, -3.4926e-02,  4.0945e-02, -3.7434e-02, -2.5890e-02,\n",
      "          4.8855e-03,  3.9086e-02, -8.4985e-03,  2.5481e-03, -3.4748e-02,\n",
      "         -2.6434e-02,  3.5378e-02,  3.6223e-03, -6.5623e-04, -9.6917e-04,\n",
      "         -2.5790e-02,  1.0904e-02,  5.9442e-03, -2.8650e-02, -6.5774e-02,\n",
      "         -3.7632e-02, -1.4367e-02, -4.3433e-02,  1.3135e-02, -4.3902e-02,\n",
      "         -4.3855e-02, -5.3940e-02, -5.0922e-02, -1.5731e-03, -2.2090e-02,\n",
      "         -5.5638e-02, -5.3228e-03, -2.0104e-02,  1.1249e-02, -4.7550e-03,\n",
      "         -2.5717e-02, -1.9691e-02, -1.9235e-02,  4.2115e-03,  2.6721e-02,\n",
      "         -4.2170e-02, -5.7505e-02,  3.5444e-02, -1.6827e-02,  1.1199e-02,\n",
      "          4.0086e-02,  3.6231e-02, -1.7821e-02,  4.0596e-02, -1.2750e-02,\n",
      "         -6.5493e-02, -2.7240e-02,  5.1912e-03, -7.5563e-02,  3.7640e-02,\n",
      "         -2.2959e-02,  2.4075e-02, -2.5400e-02,  1.4495e-02, -2.0140e-02,\n",
      "          5.8721e-03, -1.1857e-02,  1.4355e-02, -2.7523e-03,  1.1819e-03,\n",
      "          4.0320e-02, -1.9957e-02,  1.9900e-02,  2.2628e-02, -3.6986e-02,\n",
      "         -1.7364e-02, -1.7196e-02,  1.8000e-02,  1.2634e-03, -1.7457e-02,\n",
      "         -4.9469e-02, -2.9812e-02, -2.4092e-02, -8.1014e-03,  1.8621e-03,\n",
      "         -4.1688e-02,  1.0998e-03,  4.4008e-03, -2.7538e-03,  2.2173e-02,\n",
      "          6.4450e-03,  1.1470e-02, -3.1098e-02,  1.5920e-02, -2.8970e-02,\n",
      "          1.7682e-02, -2.5867e-03,  1.2408e-02, -2.6490e-02, -1.5983e-02,\n",
      "         -1.7047e-02,  3.6521e-02,  3.4072e-03,  5.8841e-03, -1.2509e-02,\n",
      "         -2.0135e-02, -2.0695e-02, -4.1008e-02, -1.7729e-03,  3.9193e-02,\n",
      "         -5.2337e-02,  1.7296e-03,  5.3429e-03, -3.7577e-02,  3.1243e-02,\n",
      "         -4.0029e-02, -2.7515e-02,  1.4920e-02, -3.2981e-02, -3.8706e-02,\n",
      "          5.0400e-03,  2.5041e-02, -3.5281e-02, -2.3244e-02,  2.2829e-02,\n",
      "         -5.7773e-03,  5.1229e-04, -3.8537e-02, -4.3080e-03,  6.9064e-04,\n",
      "          1.8014e-02,  1.9103e-02, -1.3925e-02,  2.8350e-02,  2.3603e-02,\n",
      "          5.6778e-03, -2.6047e-02,  4.0636e-03,  4.0837e-02, -4.8699e-03,\n",
      "          3.3543e-02,  3.6787e-02,  1.4992e-02,  2.9015e-02, -3.8919e-02,\n",
      "         -2.0505e-02,  4.0167e-02,  3.1395e-02, -2.9002e-02, -3.4305e-02,\n",
      "          2.7726e-02,  2.0093e-02, -7.4484e-03, -6.8409e-02,  1.4136e-02,\n",
      "         -7.3819e-02, -7.6463e-02,  2.2084e-02, -4.9484e-02, -4.9008e-02,\n",
      "         -2.6149e-02,  4.1736e-04,  1.3424e-02,  1.7441e-03, -5.0510e-02,\n",
      "         -5.9381e-02, -5.9524e-02,  9.0435e-03,  2.1536e-03,  2.4932e-02,\n",
      "          2.7481e-02, -3.9486e-02, -8.6592e-03, -1.9829e-02, -2.3569e-02,\n",
      "          1.3144e-02, -7.1447e-03, -8.7312e-03, -8.5272e-03,  2.4219e-02,\n",
      "         -4.9141e-02,  1.3644e-02,  6.5593e-03,  4.8720e-03,  1.4015e-02,\n",
      "         -2.2829e-02, -5.8140e-03,  5.2754e-03,  3.7978e-02,  3.7851e-02,\n",
      "         -2.0278e-02,  2.8699e-02,  1.9659e-02,  4.0765e-02,  2.5195e-02,\n",
      "         -3.9048e-02, -3.0634e-02, -3.8328e-02, -4.1257e-02,  4.5492e-03,\n",
      "         -3.1227e-02, -1.4712e-02, -2.7033e-02, -5.6344e-02, -5.4595e-02,\n",
      "         -2.0752e-02, -7.8258e-02, -2.9726e-02,  3.1290e-02, -3.0005e-02,\n",
      "         -5.1814e-02,  1.8093e-02, -5.2103e-02, -1.4406e-02, -1.0321e-02,\n",
      "         -2.7023e-02, -5.3274e-02, -6.2670e-02, -5.1711e-03, -2.1196e-02,\n",
      "         -1.0599e-02,  2.6089e-02, -5.6974e-02, -4.7230e-02,  8.7568e-03,\n",
      "         -2.9443e-02, -2.8537e-02, -1.7273e-02, -2.6323e-02, -7.8677e-02,\n",
      "          2.3436e-02, -2.8459e-02, -3.1864e-02, -3.6244e-02, -1.6638e-02,\n",
      "         -1.4459e-02, -2.2506e-02, -2.2680e-02,  3.2780e-02, -7.0945e-03,\n",
      "          1.9328e-02,  1.5857e-02, -8.2083e-04, -3.8769e-02,  3.7448e-02,\n",
      "         -2.5261e-02,  1.0913e-02,  3.2544e-02, -1.6640e-02,  1.1519e-02,\n",
      "         -2.0053e-02, -2.5628e-02,  1.7396e-02, -3.4236e-02,  2.8250e-02,\n",
      "          3.9742e-02, -2.7750e-02,  2.5734e-02,  2.4304e-02,  1.5046e-02,\n",
      "         -3.5678e-02,  1.1587e-02,  9.2836e-03, -2.9520e-02,  1.0855e-02,\n",
      "          1.3121e-02,  1.3660e-02,  2.4806e-02, -2.2801e-02, -3.1728e-02,\n",
      "          2.8558e-02,  3.4221e-02, -2.4019e-02, -2.4465e-02, -2.6693e-02,\n",
      "         -7.0827e-02, -7.7601e-02, -4.6776e-02, -5.8005e-02, -8.2577e-02,\n",
      "         -1.0821e-02, -8.1950e-03, -1.6797e-02,  4.7721e-03, -2.7450e-02,\n",
      "          3.3155e-03, -3.2497e-03, -3.6406e-02, -3.1724e-02, -2.9378e-02,\n",
      "         -1.0792e-03, -4.8198e-02, -4.6667e-02, -3.9067e-02, -1.4063e-02,\n",
      "         -1.6465e-02, -3.4224e-02,  3.0661e-02, -6.0789e-04,  9.2551e-03,\n",
      "         -1.0692e-02, -3.5194e-02,  3.4852e-02,  1.2115e-03, -2.5247e-02,\n",
      "          2.5834e-02, -4.1434e-02, -2.8081e-03, -5.2506e-02, -3.9619e-03,\n",
      "         -8.1263e-03, -6.9993e-03, -1.6585e-02, -1.7683e-02,  1.3479e-02,\n",
      "          1.3306e-02,  3.2747e-02,  6.8009e-04, -1.8187e-02,  9.0163e-03,\n",
      "          3.1397e-02,  1.8496e-02,  3.4644e-03, -5.4356e-02, -8.2393e-03,\n",
      "         -3.4505e-02, -7.1145e-02, -4.0923e-02, -5.3348e-02, -1.0613e-02,\n",
      "         -2.8256e-02,  2.4891e-02,  8.6506e-03, -8.4196e-03, -3.3624e-02,\n",
      "         -3.4959e-02, -6.1531e-02, -2.1544e-02, -3.8599e-02,  3.3199e-02,\n",
      "          1.3066e-02,  3.2023e-02, -3.9069e-03,  2.0857e-02, -3.0408e-02,\n",
      "         -1.9967e-02, -2.4230e-02, -2.1566e-02,  2.9637e-02,  1.5594e-02,\n",
      "         -9.1185e-03, -2.7869e-02, -3.1782e-02, -4.0663e-02, -3.7688e-02,\n",
      "         -1.8306e-02,  2.4125e-02,  3.1512e-03,  6.4462e-03,  8.0827e-03,\n",
      "         -3.8456e-02, -4.1329e-02,  1.9181e-02, -2.7469e-02, -1.9577e-02,\n",
      "          1.5632e-02, -2.4017e-03, -2.1483e-02, -1.0963e-02,  1.6056e-03,\n",
      "         -1.7881e-03, -2.1337e-02, -4.0019e-02, -4.7812e-02, -3.9887e-04,\n",
      "         -2.0380e-02, -3.6407e-02, -1.3362e-03, -3.3159e-02,  3.9031e-02,\n",
      "         -8.7130e-03,  1.5297e-02,  9.8819e-03, -1.9220e-02, -2.2446e-03,\n",
      "          2.9657e-02,  2.4378e-02, -1.4011e-02,  1.4269e-02, -2.8749e-02,\n",
      "          5.6661e-03, -1.1903e-02, -3.0326e-02, -9.1653e-03, -4.8409e-03,\n",
      "          3.1427e-03, -1.2166e-02,  1.3130e-02, -3.3045e-02, -5.1884e-02,\n",
      "          1.4728e-02, -3.7354e-02,  1.0039e-02,  2.0925e-02, -2.8046e-03,\n",
      "          2.8425e-02,  4.1554e-02,  1.5786e-02, -9.9695e-03,  7.9036e-03,\n",
      "         -1.4737e-02, -2.0889e-02,  3.6302e-02,  8.6601e-03, -1.4174e-02,\n",
      "          2.2746e-03,  3.4177e-02, -3.5805e-02, -5.6584e-03, -1.4565e-02,\n",
      "          6.2149e-03, -1.2924e-02,  1.5317e-02,  1.4325e-02, -2.6560e-02,\n",
      "          1.1594e-03, -3.0889e-02, -4.1520e-02,  1.0775e-02,  1.0850e-02,\n",
      "          1.1716e-02,  2.8081e-03,  2.1603e-03, -9.4786e-03,  6.3370e-03,\n",
      "          3.0100e-02, -5.3193e-04, -6.8905e-02, -6.4636e-02, -2.8302e-02,\n",
      "         -4.1184e-02, -1.1330e-02,  2.7826e-02,  3.2233e-02, -3.9972e-02,\n",
      "         -3.9478e-02, -2.8262e-02,  1.8508e-02,  7.9223e-03,  1.6220e-03,\n",
      "          1.8213e-02,  3.9295e-02, -2.4669e-02,  1.3552e-03, -2.0706e-02,\n",
      "         -1.7318e-02, -1.2047e-02, -2.8557e-02,  2.7891e-02,  7.4434e-03,\n",
      "          2.5483e-02, -2.9054e-02, -7.1259e-03, -3.1896e-02,  1.2724e-02,\n",
      "         -1.5179e-02,  3.0604e-03, -4.0424e-02,  3.3025e-02,  6.2875e-03,\n",
      "          4.1445e-02,  2.4264e-02,  2.3710e-02,  7.1586e-03,  2.5604e-02,\n",
      "          4.0473e-02,  1.4774e-03,  1.0064e-02, -2.5704e-03, -1.8879e-02,\n",
      "         -2.4023e-02,  4.1217e-02, -3.4809e-02,  1.8250e-02,  3.9871e-02,\n",
      "         -3.8625e-02, -2.4198e-02,  3.2324e-02,  1.0104e-02,  9.8866e-03,\n",
      "         -2.1098e-02, -3.0082e-02,  3.3524e-02, -2.6516e-02, -2.4819e-04,\n",
      "          1.3863e-02,  3.2579e-02,  2.6856e-02,  1.0142e-02, -2.8252e-02,\n",
      "         -1.9285e-02, -1.7671e-02, -3.3707e-02, -6.5855e-03, -1.2545e-02,\n",
      "          3.6736e-03, -3.8936e-02, -4.0950e-02, -7.8247e-02, -3.3486e-02,\n",
      "         -7.0656e-02, -5.4056e-02, -3.2109e-02, -8.6416e-02, -2.6503e-02,\n",
      "         -6.7945e-02, -8.7346e-02, -1.2566e-02, -3.8652e-02, -2.6418e-02,\n",
      "          3.2803e-02, -9.8258e-04,  9.0259e-03, -3.6170e-02, -3.3249e-02,\n",
      "         -2.3552e-02,  1.7052e-02, -6.1076e-02, -6.3915e-02, -3.4106e-02,\n",
      "         -1.5264e-02, -6.9266e-02, -8.6055e-02, -1.1755e-02, -1.4379e-02,\n",
      "         -2.6991e-02, -3.5692e-02,  1.2034e-02, -3.8007e-02,  3.4140e-02,\n",
      "         -4.3766e-02,  2.4202e-02,  1.5799e-02,  1.3075e-02,  3.0548e-02,\n",
      "          1.0903e-02,  3.0634e-02,  1.5952e-02, -4.0517e-02,  3.4147e-03,\n",
      "         -2.9233e-02, -2.6350e-02, -8.1356e-03,  3.2664e-02,  3.2111e-02,\n",
      "         -5.9701e-02, -4.7059e-02,  1.5450e-02,  1.1175e-02, -3.4174e-03,\n",
      "         -4.8803e-03, -1.0578e-02, -1.3457e-02,  1.0080e-03, -1.7375e-02,\n",
      "         -6.5560e-03, -8.1077e-03, -4.0051e-02, -3.1561e-02,  2.2515e-02,\n",
      "          3.7135e-03, -3.3372e-02, -1.9867e-02, -9.0298e-03, -5.0228e-05,\n",
      "          3.5408e-02,  7.9690e-03,  3.5245e-02,  1.8890e-03, -4.9885e-03,\n",
      "         -3.3695e-02]])\n",
      "tensor([-0.0380])\n"
     ]
    }
   ],
   "source": [
    "for param in model2.parameters():\n",
    "\n",
    "        print(param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from custom_bnn import CBNNConv2d\n",
    "class SimpleNet2(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(SimpleNet2, self).__init__()\n",
    "        self.binarized_conv1 = CBNNConv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.binarized_conv2 = CBNNConv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(25088, 10)  # Flattened 3x3 output to a single scalar\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.binarized_conv1(x))\n",
    "        x = F.relu(self.binarized_conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "model =SimpleNet2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from custom_bnn import BNNLinear\n",
    "class BinaryLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(BinaryLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.randn(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.zeros(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Binarize the weights\n",
    "        binary_weight = torch.sign(self.weight)\n",
    "        # Apply the linear transformation\n",
    "        x = F.linear(x, binary_weight, self.bias)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class BNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BNNNet, self).__init__()\n",
    "        self.fc1 = BNNLinear(28 * 28, 512)\n",
    "        self.fc2 = BNNLinear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the MNIST images\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation at the output layer\n",
    "        return x \n",
    "model =BNNNet()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# Training settings\n",
    "batch_size = 256\n",
    "learning_rate = .0001\n",
    "epochs = 10\n",
    "\n",
    "# Prepare MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the full dataset\n",
    "full_dataset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "\n",
    "# Define dataset split ratios\n",
    "train_size = int(0.8 * len(full_dataset))  # 80% for training\n",
    "val_size = len(full_dataset) - train_size  # The rest for validation\n",
    "\n",
    "# Split dataset\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)  \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "\n",
    "def evaluate(model, device, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data) \n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / total\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model for Epoch 1\n",
      "Train Epoch: 1 [0/48000 (0%)]\tLoss: 0.383495\n",
      "Train Epoch: 1 [25600/48000 (53%)]\tLoss: 0.826729\n",
      "\n",
      "Evaluating Model on Validation Set for Epoch 1\n",
      "Validation Accuracy: 86.12%\n",
      "Validation Accuracy: 87.16%\n",
      "\n",
      "Training Model for Epoch 2\n",
      "Train Epoch: 2 [0/48000 (0%)]\tLoss: 0.463413\n",
      "Train Epoch: 2 [25600/48000 (53%)]\tLoss: 1.074702\n",
      "\n",
      "Evaluating Model on Validation Set for Epoch 2\n",
      "Validation Accuracy: 65.40%\n",
      "Validation Accuracy: 66.63%\n",
      "\n",
      "Training Model for Epoch 3\n",
      "Train Epoch: 3 [0/48000 (0%)]\tLoss: 1.194150\n",
      "Train Epoch: 3 [25600/48000 (53%)]\tLoss: 0.323646\n",
      "\n",
      "Evaluating Model on Validation Set for Epoch 3\n",
      "Validation Accuracy: 87.87%\n",
      "Validation Accuracy: 89.17%\n",
      "\n",
      "Training Model for Epoch 4\n",
      "Train Epoch: 4 [0/48000 (0%)]\tLoss: 0.315208\n",
      "Train Epoch: 4 [25600/48000 (53%)]\tLoss: 0.574047\n",
      "\n",
      "Evaluating Model on Validation Set for Epoch 4\n",
      "Validation Accuracy: 88.34%\n",
      "Validation Accuracy: 89.40%\n",
      "\n",
      "Training Model for Epoch 5\n",
      "Train Epoch: 5 [0/48000 (0%)]\tLoss: 0.247232\n",
      "Train Epoch: 5 [25600/48000 (53%)]\tLoss: 0.443924\n",
      "\n",
      "Evaluating Model on Validation Set for Epoch 5\n",
      "Validation Accuracy: 87.65%\n",
      "Validation Accuracy: 88.83%\n",
      "\n",
      "Training Model for Epoch 6\n",
      "Train Epoch: 6 [0/48000 (0%)]\tLoss: 0.358736\n",
      "Train Epoch: 6 [25600/48000 (53%)]\tLoss: 1.046851\n",
      "\n",
      "Evaluating Model on Validation Set for Epoch 6\n",
      "Validation Accuracy: 88.92%\n",
      "Validation Accuracy: 90.05%\n",
      "\n",
      "Training Model for Epoch 7\n",
      "Train Epoch: 7 [0/48000 (0%)]\tLoss: 0.311079\n",
      "Train Epoch: 7 [25600/48000 (53%)]\tLoss: 0.285834\n",
      "\n",
      "Evaluating Model on Validation Set for Epoch 7\n",
      "Validation Accuracy: 88.61%\n",
      "Validation Accuracy: 89.62%\n",
      "\n",
      "Training Model for Epoch 8\n",
      "Train Epoch: 8 [0/48000 (0%)]\tLoss: 0.393650\n",
      "Train Epoch: 8 [25600/48000 (53%)]\tLoss: 0.325836\n",
      "\n",
      "Evaluating Model on Validation Set for Epoch 8\n",
      "Validation Accuracy: 89.01%\n",
      "Validation Accuracy: 90.45%\n",
      "\n",
      "Training Model for Epoch 9\n",
      "Train Epoch: 9 [0/48000 (0%)]\tLoss: 0.378443\n",
      "Train Epoch: 9 [25600/48000 (53%)]\tLoss: 0.374106\n",
      "\n",
      "Evaluating Model on Validation Set for Epoch 9\n",
      "Validation Accuracy: 90.11%\n",
      "Validation Accuracy: 91.43%\n",
      "\n",
      "Training Model for Epoch 10\n",
      "Train Epoch: 10 [0/48000 (0%)]\tLoss: 2.573399\n",
      "Train Epoch: 10 [25600/48000 (53%)]\tLoss: 0.342108\n",
      "\n",
      "Evaluating Model on Validation Set for Epoch 10\n",
      "Validation Accuracy: 79.64%\n",
      "Validation Accuracy: 80.82%\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "# optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "lr=.00001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"\\nTraining Model for Epoch {epoch}\")\n",
    "    train(model, device, train_loader, optimizer, criterion)\n",
    "    \n",
    "    print(f\"\\nEvaluating Model on Validation Set for Epoch {epoch}\")\n",
    "    accuracy = evaluate(model, device, val_loader)\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')\n",
    "    accuracy = evaluate(model, device, train_loader)\n",
    "    print(f'Validation Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ResNet34:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"layer1.0.conv1.weight\", \"layer1.0.conv2.weight\", \"layer1.1.conv1.weight\", \"layer1.1.conv2.weight\", \"layer1.2.conv1.weight\", \"layer1.2.conv2.weight\", \"layer2.0.conv1.weight\", \"layer2.0.conv2.weight\", \"layer2.0.downsample.0.weight\", \"layer2.1.conv1.weight\", \"layer2.1.conv2.weight\", \"layer2.2.conv1.weight\", \"layer2.2.conv2.weight\", \"layer2.3.conv1.weight\", \"layer2.3.conv2.weight\", \"layer3.0.conv1.weight\", \"layer3.0.conv2.weight\", \"layer3.0.downsample.0.weight\", \"layer3.1.conv1.weight\", \"layer3.1.conv2.weight\", \"layer3.2.conv1.weight\", \"layer3.2.conv2.weight\", \"layer3.3.conv1.weight\", \"layer3.3.conv2.weight\", \"layer3.4.conv1.weight\", \"layer3.4.conv2.weight\", \"layer3.5.conv1.weight\", \"layer3.5.conv2.weight\", \"layer4.0.conv1.weight\", \"layer4.0.conv2.weight\", \"layer4.0.downsample.0.weight\", \"layer4.1.conv1.weight\", \"layer4.1.conv2.weight\", \"layer4.2.conv1.weight\", \"layer4.2.conv2.weight\". \n\tUnexpected key(s) in state_dict: \"conv1.conv.weight\", \"conv1.conv.bias\", \"layer1.0.conv1.conv.weight\", \"layer1.0.conv1.conv.bias\", \"layer1.0.conv2.conv.weight\", \"layer1.0.conv2.conv.bias\", \"layer1.1.conv1.conv.weight\", \"layer1.1.conv1.conv.bias\", \"layer1.1.conv2.conv.weight\", \"layer1.1.conv2.conv.bias\", \"layer1.2.conv1.conv.weight\", \"layer1.2.conv1.conv.bias\", \"layer1.2.conv2.conv.weight\", \"layer1.2.conv2.conv.bias\", \"layer2.0.conv1.conv.weight\", \"layer2.0.conv1.conv.bias\", \"layer2.0.conv2.conv.weight\", \"layer2.0.conv2.conv.bias\", \"layer2.0.downsample.0.conv.weight\", \"layer2.0.downsample.0.conv.bias\", \"layer2.1.conv1.conv.weight\", \"layer2.1.conv1.conv.bias\", \"layer2.1.conv2.conv.weight\", \"layer2.1.conv2.conv.bias\", \"layer2.2.conv1.conv.weight\", \"layer2.2.conv1.conv.bias\", \"layer2.2.conv2.conv.weight\", \"layer2.2.conv2.conv.bias\", \"layer2.3.conv1.conv.weight\", \"layer2.3.conv1.conv.bias\", \"layer2.3.conv2.conv.weight\", \"layer2.3.conv2.conv.bias\", \"layer3.0.conv1.conv.weight\", \"layer3.0.conv1.conv.bias\", \"layer3.0.conv2.conv.weight\", \"layer3.0.conv2.conv.bias\", \"layer3.0.downsample.0.conv.weight\", \"layer3.0.downsample.0.conv.bias\", \"layer3.1.conv1.conv.weight\", \"layer3.1.conv1.conv.bias\", \"layer3.1.conv2.conv.weight\", \"layer3.1.conv2.conv.bias\", \"layer3.2.conv1.conv.weight\", \"layer3.2.conv1.conv.bias\", \"layer3.2.conv2.conv.weight\", \"layer3.2.conv2.conv.bias\", \"layer3.3.conv1.conv.weight\", \"layer3.3.conv1.conv.bias\", \"layer3.3.conv2.conv.weight\", \"layer3.3.conv2.conv.bias\", \"layer3.4.conv1.conv.weight\", \"layer3.4.conv1.conv.bias\", \"layer3.4.conv2.conv.weight\", \"layer3.4.conv2.conv.bias\", \"layer3.5.conv1.conv.weight\", \"layer3.5.conv1.conv.bias\", \"layer3.5.conv2.conv.weight\", \"layer3.5.conv2.conv.bias\", \"layer4.0.conv1.conv.weight\", \"layer4.0.conv1.conv.bias\", \"layer4.0.conv2.conv.weight\", \"layer4.0.conv2.conv.bias\", \"layer4.0.downsample.0.conv.weight\", \"layer4.0.downsample.0.conv.bias\", \"layer4.1.conv1.conv.weight\", \"layer4.1.conv1.conv.bias\", \"layer4.1.conv2.conv.weight\", \"layer4.1.conv2.conv.bias\", \"layer4.2.conv1.conv.weight\", \"layer4.2.conv1.conv.bias\", \"layer4.2.conv2.conv.weight\", \"layer4.2.conv2.conv.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:2152\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2147\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2148\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2149\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2152\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2153\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ResNet34:\n\tMissing key(s) in state_dict: \"conv1.weight\", \"layer1.0.conv1.weight\", \"layer1.0.conv2.weight\", \"layer1.1.conv1.weight\", \"layer1.1.conv2.weight\", \"layer1.2.conv1.weight\", \"layer1.2.conv2.weight\", \"layer2.0.conv1.weight\", \"layer2.0.conv2.weight\", \"layer2.0.downsample.0.weight\", \"layer2.1.conv1.weight\", \"layer2.1.conv2.weight\", \"layer2.2.conv1.weight\", \"layer2.2.conv2.weight\", \"layer2.3.conv1.weight\", \"layer2.3.conv2.weight\", \"layer3.0.conv1.weight\", \"layer3.0.conv2.weight\", \"layer3.0.downsample.0.weight\", \"layer3.1.conv1.weight\", \"layer3.1.conv2.weight\", \"layer3.2.conv1.weight\", \"layer3.2.conv2.weight\", \"layer3.3.conv1.weight\", \"layer3.3.conv2.weight\", \"layer3.4.conv1.weight\", \"layer3.4.conv2.weight\", \"layer3.5.conv1.weight\", \"layer3.5.conv2.weight\", \"layer4.0.conv1.weight\", \"layer4.0.conv2.weight\", \"layer4.0.downsample.0.weight\", \"layer4.1.conv1.weight\", \"layer4.1.conv2.weight\", \"layer4.2.conv1.weight\", \"layer4.2.conv2.weight\". \n\tUnexpected key(s) in state_dict: \"conv1.conv.weight\", \"conv1.conv.bias\", \"layer1.0.conv1.conv.weight\", \"layer1.0.conv1.conv.bias\", \"layer1.0.conv2.conv.weight\", \"layer1.0.conv2.conv.bias\", \"layer1.1.conv1.conv.weight\", \"layer1.1.conv1.conv.bias\", \"layer1.1.conv2.conv.weight\", \"layer1.1.conv2.conv.bias\", \"layer1.2.conv1.conv.weight\", \"layer1.2.conv1.conv.bias\", \"layer1.2.conv2.conv.weight\", \"layer1.2.conv2.conv.bias\", \"layer2.0.conv1.conv.weight\", \"layer2.0.conv1.conv.bias\", \"layer2.0.conv2.conv.weight\", \"layer2.0.conv2.conv.bias\", \"layer2.0.downsample.0.conv.weight\", \"layer2.0.downsample.0.conv.bias\", \"layer2.1.conv1.conv.weight\", \"layer2.1.conv1.conv.bias\", \"layer2.1.conv2.conv.weight\", \"layer2.1.conv2.conv.bias\", \"layer2.2.conv1.conv.weight\", \"layer2.2.conv1.conv.bias\", \"layer2.2.conv2.conv.weight\", \"layer2.2.conv2.conv.bias\", \"layer2.3.conv1.conv.weight\", \"layer2.3.conv1.conv.bias\", \"layer2.3.conv2.conv.weight\", \"layer2.3.conv2.conv.bias\", \"layer3.0.conv1.conv.weight\", \"layer3.0.conv1.conv.bias\", \"layer3.0.conv2.conv.weight\", \"layer3.0.conv2.conv.bias\", \"layer3.0.downsample.0.conv.weight\", \"layer3.0.downsample.0.conv.bias\", \"layer3.1.conv1.conv.weight\", \"layer3.1.conv1.conv.bias\", \"layer3.1.conv2.conv.weight\", \"layer3.1.conv2.conv.bias\", \"layer3.2.conv1.conv.weight\", \"layer3.2.conv1.conv.bias\", \"layer3.2.conv2.conv.weight\", \"layer3.2.conv2.conv.bias\", \"layer3.3.conv1.conv.weight\", \"layer3.3.conv1.conv.bias\", \"layer3.3.conv2.conv.weight\", \"layer3.3.conv2.conv.bias\", \"layer3.4.conv1.conv.weight\", \"layer3.4.conv1.conv.bias\", \"layer3.4.conv2.conv.weight\", \"layer3.4.conv2.conv.bias\", \"layer3.5.conv1.conv.weight\", \"layer3.5.conv1.conv.bias\", \"layer3.5.conv2.conv.weight\", \"layer3.5.conv2.conv.bias\", \"layer4.0.conv1.conv.weight\", \"layer4.0.conv1.conv.bias\", \"layer4.0.conv2.conv.weight\", \"layer4.0.conv2.conv.bias\", \"layer4.0.downsample.0.conv.weight\", \"layer4.0.downsample.0.conv.bias\", \"layer4.1.conv1.conv.weight\", \"layer4.1.conv1.conv.bias\", \"layer4.1.conv2.conv.weight\", \"layer4.1.conv2.conv.bias\", \"layer4.2.conv1.conv.weight\", \"layer4.2.conv1.conv.bias\", \"layer4.2.conv2.conv.weight\", \"layer4.2.conv2.conv.bias\". "
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Start running, host: Desktop@HPDESKTOP, work_dir: d:\\study\\S3\\Project thesis\\Work\\checkpoints\n",
      "INFO:root:Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      " -------------------- \n",
      "INFO:root:workflow: [('train', 1)], max: 10 epochs\n",
      "INFO:root:Checkpoints will be saved to d:\\study\\S3\\Project thesis\\Work\\checkpoints by HardDiskBackend.\n",
      "INFO:root:Saving checkpoint at 1 epochs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m runner\u001b[38;5;241m.\u001b[39mregister_hook(checkpoint_hook)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Training loop configuration\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Data loaders for training\u001b[39;49;00m\n\u001b[0;32m     66\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Workflow (train for 1 epoch at a time)\u001b[39;49;00m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Total number of epochs\u001b[39;49;00m\n\u001b[0;32m     68\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# To manually save a checkpoint at any time, you can call:\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# runner.save_checkpoint('./checkpoints', filename_tmpl='epoch_{}.pth')\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\runner\\epoch_based_runner.py:130\u001b[0m, in \u001b[0;36mEpochBasedRunner.run\u001b[1;34m(self, data_loaders, workflow, max_epochs, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_epochs:\n\u001b[0;32m    129\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m             \u001b[43mepoch_runner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# wait for some hooks like loggers to finish\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_hook(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter_run\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\runner\\epoch_based_runner.py:56\u001b[0m, in \u001b[0;36mEpochBasedRunner.train\u001b[1;34m(self, data_loader, **kwargs)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_batch\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mafter_train_epoch\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_epoch \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\runner\\base_runner.py:309\u001b[0m, in \u001b[0;36mBaseRunner.call_hook\u001b[1;34m(self, fn_name)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call all hooks.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \n\u001b[0;32m    304\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    fn_name (str): The function name in each hook to be called, such as\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m        \"before_train_epoch\".\u001b[39;00m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hooks:\n\u001b[1;32m--> 309\u001b[0m     \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\runner\\hooks\\checkpoint.py:116\u001b[0m, in \u001b[0;36mCheckpointHook.after_train_epoch\u001b[1;34m(self, runner)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msync_buffer:\n\u001b[0;32m    115\u001b[0m     allreduce_params(runner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbuffers())\n\u001b[1;32m--> 116\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\runner\\dist_utils.py:135\u001b[0m, in \u001b[0;36mmaster_only.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    133\u001b[0m rank, _ \u001b[38;5;241m=\u001b[39m get_dist_info()\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\runner\\hooks\\checkpoint.py:121\u001b[0m, in \u001b[0;36mCheckpointHook._save_checkpoint\u001b[1;34m(self, runner)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;129m@master_only\u001b[39m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_save_checkpoint\u001b[39m(\u001b[38;5;28mself\u001b[39m, runner):\n\u001b[0;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save the current checkpoint and delete unwanted checkpoint.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m     \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_optimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mmeta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mby_epoch:\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\runner\\epoch_based_runner.py:172\u001b[0m, in \u001b[0;36mEpochBasedRunner.save_checkpoint\u001b[1;34m(self, out_dir, filename_tmpl, save_optimizer, meta, create_symlink)\u001b[0m\n\u001b[0;32m    170\u001b[0m filepath \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mjoin(out_dir, filename)\n\u001b[0;32m    171\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;28;01mif\u001b[39;00m save_optimizer \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 172\u001b[0m \u001b[43msave_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# in some environments, `os.symlink` is not supported, you may need to\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;66;03m# set `create_symlink` to False\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m create_symlink:\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\runner\\checkpoint.py:800\u001b[0m, in \u001b[0;36msave_checkpoint\u001b[1;34m(model, filename, optimizer, meta, file_client_args)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    799\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(checkpoint, f)\n\u001b[1;32m--> 800\u001b[0m     \u001b[43mfile_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\fileio\\file_client.py:1040\u001b[0m, in \u001b[0;36mFileClient.put\u001b[1;34m(self, obj, filepath)\u001b[0m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mput\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;28mbytes\u001b[39m, filepath: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1030\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write data to a given ``filepath`` with 'wb' mode.\u001b[39;00m\n\u001b[0;32m   1031\u001b[0m \n\u001b[0;32m   1032\u001b[0m \u001b[38;5;124;03m    Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;124;03m        filepath (str or Path): Path to write data.\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\fileio\\file_client.py:567\u001b[0m, in \u001b[0;36mHardDiskBackend.put\u001b[1;34m(self, obj, filepath)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mput\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: \u001b[38;5;28mbytes\u001b[39m, filepath: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    557\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Write data to a given ``filepath`` with 'wb' mode.\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m    Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03m        filepath (str or Path): Path to write data.\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 567\u001b[0m     \u001b[43mmmcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmkdir_or_exist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mosp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdirname\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    569\u001b[0m         f\u001b[38;5;241m.\u001b[39mwrite(obj)\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\site-packages\\mmcv\\utils\\path.py:30\u001b[0m, in \u001b[0;36mmkdir_or_exist\u001b[1;34m(dir_name, mode)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     29\u001b[0m dir_name \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mexpanduser(dir_name)\n\u001b[1;32m---> 30\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\os.py:211\u001b[0m, in \u001b[0;36mmakedirs\u001b[1;34m(name, mode, exist_ok)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tail:\n\u001b[0;32m    210\u001b[0m     head, tail \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39msplit(head)\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    213\u001b[0m         makedirs(head, exist_ok\u001b[38;5;241m=\u001b[39mexist_ok)\n",
      "File \u001b[1;32mc:\\Users\\Desktop\\anaconda3\\envs\\bibench\\lib\\genericpath.py:19\u001b[0m, in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 19\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mOSError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from mmcv.runner import EpochBasedRunner, CheckpointHook\n",
    "from bnn import BNNConv2d\n",
    "import logging \n",
    "\n",
    "class SimpleNet2(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(SimpleNet2, self).__init__()\n",
    "        self.binarized_conv1 = BNNConv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.binarized_conv2 = BNNConv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(25088, 10)  # Adjust input size to match your actual input\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.binarized_conv1(x))\n",
    "        x = F.relu(self.binarized_conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    def train_step(self, data, optimizer, **kwargs):\n",
    "        \"\"\"\n",
    "        The `train_step` method that MMCV's runner will call during training.\n",
    "        \"\"\"\n",
    "        self.train()  # Set the model to training mode\n",
    "        inputs, labels = data\n",
    "        outputs = self(inputs)\n",
    "        \n",
    "        # Assume CrossEntropyLoss is used\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # MMCV expects a dict with 'loss' key for logging\n",
    "        return {'loss': loss.item()}\n",
    "\n",
    "# Instantiate the model, optimizer, and criterion\n",
    "model = SimpleNet2()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "logging.basicConfig(level=logging.INFO)  # You can change the level to DEBUG if you want more details\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Dummy data loader for illustration\n",
    "# Replace with your actual data loader\n",
    "train_loader = [(torch.randn(32, 1, 28, 28), torch.randint(0, 10, (32,))) for _ in range(100)]\n",
    "\n",
    "# Runner configuration\n",
    "runner = EpochBasedRunner(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    work_dir='./checkpoints',  # Directory to save checkpoints\n",
    "    logger=logger\n",
    ")\n",
    "\n",
    "# Register a checkpoint hook\n",
    "checkpoint_hook = CheckpointHook(interval=1, max_keep_ckpts=5)\n",
    "runner.register_hook(checkpoint_hook)\n",
    "\n",
    "# Training loop configuration\n",
    "runner.run(\n",
    "    data_loaders=[train_loader],  # Data loaders for training\n",
    "    workflow=[('train', 1)],  # Workflow (train for 1 epoch at a time)\n",
    "    max_epochs=10  # Total number of epochs\n",
    ")\n",
    "\n",
    "# To manually save a checkpoint at any time, you can call:\n",
    "# runner.save_checkpoint('./checkpoints', filename_tmpl='epoch_{}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcv.runner import load_checkpoint\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from custom_bnn import CBNNConv2d\n",
    "class SimpleNet2(nn.Module): \n",
    "    def __init__(self):\n",
    "        super(SimpleNet2, self).__init__()\n",
    "        self.binarized_conv1 = CBNNConv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.binarized_conv2 = CBNNConv2d(in_channels=64, out_channels=32, kernel_size=3, padding=1)\n",
    "        self.fc = nn.Linear(25088, 10)  # Flattened 3x3 output to a single scalar\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.binarized_conv1(x))\n",
    "        x = F.relu(self.binarized_conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "model=SimpleNet2()\n",
    "# # Load the checkpoint\n",
    "# checkpoint_path = './checkpoints/latest.pth'\n",
    "# load_checkpoint(model, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SimpleNet2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofiler\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mprofiler\u001b[39;00m\n\u001b[0;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSimpleNet2\u001b[49m()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      6\u001b[0m data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m)  \u001b[38;5;66;03m# Batch size of 32, 10 features\u001b[39;00m\n\u001b[0;32m      7\u001b[0m data\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'SimpleNet2' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.profiler as profiler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SimpleNet2().to(device)\n",
    "data = torch.randn(256,1,28,28)  # Batch size of 32, 10 features\n",
    "data=data.to(device)\n",
    "# Define a function to perform a forward pass\n",
    "def forward_pass():\n",
    "    model(data)\n",
    "\n",
    "# Set up the profiler\n",
    "with profiler.profile(\n",
    "    activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],  # Include CUDA if using GPU\n",
    "    record_shapes=True,  # Record tensor shapes\n",
    "    with_stack=True  # Record stack traces\n",
    ") as prof:\n",
    "    forward_pass()  # Run the forward pass inside the profiler\n",
    "\n",
    "# Print the profiling results\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    1904 KB |  400731 KB |  747534 KB |  745629 KB |\n",
      "|       from large pool |       0 KB |  397824 KB |  741378 KB |  741378 KB |\n",
      "|       from small pool |    1904 KB |    4978 KB |    6155 KB |    4251 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    1904 KB |  400731 KB |  747534 KB |  745629 KB |\n",
      "|       from large pool |       0 KB |  397824 KB |  741378 KB |  741378 KB |\n",
      "|       from small pool |    1904 KB |    4978 KB |    6155 KB |    4251 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  471040 KB |  471040 KB |  471040 KB |       0 B  |\n",
      "|       from large pool |  464896 KB |  464896 KB |  464896 KB |       0 B  |\n",
      "|       from small pool |    6144 KB |    6144 KB |    6144 KB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |    2191 KB |  140525 KB |  309902 KB |  307710 KB |\n",
      "|       from large pool |       0 KB |  139264 KB |  300029 KB |  300029 KB |\n",
      "|       from small pool |    2191 KB |    2726 KB |    9872 KB |    7681 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |       9    |      20    |      70    |      61    |\n",
      "|       from large pool |       0    |       6    |      14    |      14    |\n",
      "|       from small pool |       9    |      18    |      56    |      47    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |       9    |      20    |      70    |      61    |\n",
      "|       from large pool |       0    |       6    |      14    |      14    |\n",
      "|       from small pool |       9    |      18    |      56    |      47    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       8    |       8    |       8    |       0    |\n",
      "|       from large pool |       5    |       5    |       5    |       0    |\n",
      "|       from small pool |       3    |       3    |       3    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       5    |      10    |      24    |      19    |\n",
      "|       from large pool |       0    |       3    |       5    |       5    |\n",
      "|       from small pool |       5    |       8    |      19    |      14    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.profiler as profiler\n",
    "def train_one_step(model, optimizer, criterion, inputs, targets):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "def profile_training_step(model, optimizer, criterion, inputs, targets):\n",
    "    with profiler.profile(\n",
    "        activities=[profiler.ProfilerActivity.CPU, profiler.ProfilerActivity.CUDA],\n",
    "        record_shapes=True,\n",
    "        with_stack=True\n",
    "    ) as prof:\n",
    "        for _ in range(6):  # Warmup and then profiling\n",
    "            train_one_step(model, optimizer, criterion, inputs, targets)\n",
    "\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_time_total\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                aten::cudnn_convolution        29.21%     824.485ms        29.21%     824.485ms       3.817ms     766.922ms        25.14%     766.922ms       3.551ms           216  \n",
      "                             aten::convolution_backward         6.09%     171.925ms         6.16%     173.835ms     804.792us     369.656ms        12.12%     373.035ms       1.727ms           216  \n",
      "                               aten::linalg_vector_norm         1.82%      51.366ms         1.82%      51.502ms       1.030ms     298.936ms         9.80%     299.109ms       5.982ms            50  \n",
      "                                              aten::sub         1.56%      43.913ms         1.56%      43.913ms      91.106us     282.486ms         9.26%     282.486ms     586.071us           482  \n",
      "                                       aten::unique_dim        14.42%     407.013ms        18.80%     530.495ms      14.736ms     198.557ms         6.51%     329.442ms       9.151ms            36  \n",
      "                                             aten::sort         4.61%     130.131ms         5.12%     144.380ms       4.125ms     125.285ms         4.11%     143.248ms       4.093ms            35  \n",
      "                                     aten::index_select         3.06%      86.287ms         3.08%      86.893ms       2.414ms      86.185ms         2.83%      88.120ms       2.448ms            36  \n",
      "                                 aten::cudnn_batch_norm         2.35%      66.413ms         2.67%      75.472ms     349.407us      49.904ms         1.64%      75.983ms     351.773us           216  \n",
      "                                            aten::empty         0.44%      12.415ms         0.44%      12.415ms       4.045us      45.966ms         1.51%      45.966ms      14.978us          3069  \n",
      "                        aten::cudnn_batch_norm_backward         2.29%      64.689ms         2.41%      67.999ms     314.810us      45.876ms         1.50%      57.165ms     264.653us           216  \n",
      "                                            aten::index         1.78%      50.339ms         2.07%      58.306ms     232.295us      45.043ms         1.48%      59.594ms     237.426us           251  \n",
      "                                            aten::clamp         1.58%      44.454ms         1.58%      44.464ms     102.926us      38.123ms         1.25%      41.989ms      97.197us           432  \n",
      "                                             aten::sign         1.04%      29.477ms         1.04%      29.477ms     116.972us      32.532ms         1.07%      32.532ms     129.095us           252  \n",
      "                                              aten::sum         1.11%      31.298ms         1.11%      31.298ms       5.216ms      31.329ms         1.03%      31.329ms       5.221ms             6  \n",
      "                                    aten::_foreach_add_         1.28%      36.195ms         1.29%      36.341ms       6.057ms      30.210ms         0.99%      40.845ms       6.808ms             6  \n",
      "                                           aten::arange         1.04%      29.213ms         1.86%      52.410ms     249.571us      26.867ms         0.88%      56.828ms     270.610us           210  \n",
      "                                      aten::logical_and         0.97%      27.367ms         0.97%      27.367ms      64.242us      26.736ms         0.88%      26.736ms      62.761us           426  \n",
      "                                           aten::detach         1.17%      32.974ms         1.31%      36.900ms      24.213us      26.498ms         0.87%      38.260ms      25.105us          1524  \n",
      "                                              aten::cat         0.82%      23.125ms         0.82%      23.125ms       2.891ms      22.979ms         0.75%      22.979ms       2.872ms             8  \n",
      "                                           aten::argmin         0.49%      13.939ms         0.50%      14.034ms     280.680us      22.693ms         0.74%      22.808ms     456.160us            50  \n",
      "                                            aten::where         1.10%      30.934ms         1.19%      33.483ms      78.599us      21.855ms         0.72%      27.902ms      65.498us           426  \n",
      "                                            aten::copy_         0.67%      18.816ms         0.67%      18.816ms      51.270us      21.541ms         0.71%      21.541ms      58.695us           367  \n",
      "                                     aten::_log_softmax         0.69%      19.410ms         0.69%      19.410ms       3.235ms      19.345ms         0.63%      19.345ms       3.224ms             6  \n",
      "                                            aten::addmm         0.70%      19.704ms         0.70%      19.881ms       3.313ms      19.260ms         0.63%      19.461ms       3.244ms             6  \n",
      "                                               aten::ge         0.74%      20.861ms         0.74%      20.861ms      48.969us      17.564ms         0.58%      17.564ms      41.230us           426  \n",
      "                                              aten::add         0.50%      14.187ms         0.50%      14.187ms      32.840us      17.286ms         0.57%      17.286ms      40.014us           432  \n",
      "                                             aten::view         0.31%       8.637ms         0.31%       8.637ms       5.388us      17.267ms         0.57%      17.267ms      10.772us          1603  \n",
      "                                         ClampBackward1         1.97%      55.499ms         6.90%     194.775ms     457.218us      16.936ms         0.56%     136.412ms     320.216us           426  \n",
      "                               aten::threshold_backward         0.53%      14.836ms         0.53%      14.836ms      74.929us      15.573ms         0.51%      15.573ms      78.652us           198  \n",
      "                                               aten::to         0.27%       7.708ms         1.24%      34.878ms      34.329us      15.360ms         0.50%      37.035ms      36.452us          1016  \n",
      "                                            aten::fill_         0.54%      15.200ms         0.54%      15.200ms      31.148us      14.201ms         0.47%      14.201ms      29.100us           488  \n",
      "                                              aten::div         0.45%      12.618ms         0.45%      12.618ms       2.103ms      12.127ms         0.40%      12.127ms       2.021ms             6  \n",
      "                                                 detach         0.13%       3.713ms         0.13%       3.713ms       2.570us      11.762ms         0.39%      11.762ms       8.140us          1445  \n",
      "                                       aten::empty_like         0.16%       4.638ms         0.23%       6.478ms      29.180us      11.720ms         0.38%      14.551ms      65.545us           222  \n",
      "                                       aten::as_strided         0.08%       2.230ms         0.08%       2.230ms       2.819us      11.147ms         0.37%      11.147ms      14.092us           791  \n",
      "                                      aten::result_type         0.01%     146.000us         0.01%     146.000us       0.225us      10.635ms         0.35%      10.635ms      16.412us           648  \n",
      "                                          aten::resize_         0.15%       4.370ms         0.15%       4.370ms       7.071us      10.505ms         0.34%      10.505ms      16.998us           618  \n",
      "                                     aten::logical_and_         0.39%      10.995ms         1.36%      38.362ms      90.052us      10.241ms         0.34%      36.977ms      86.800us           426  \n",
      "autograd::engine::evaluate_function: torch::autograd...         0.69%      19.409ms         2.01%      56.807ms      86.071us      10.197ms         0.33%      33.977ms      51.480us           660  \n",
      "                                          aten::reshape         0.43%      12.267ms         0.49%      13.803ms      29.182us      10.161ms         0.33%      14.154ms      29.924us           473  \n",
      "                                    aten::scalar_tensor         0.91%      25.661ms         1.28%      36.100ms      84.742us       9.805ms         0.32%      29.228ms      68.610us           426  \n",
      "                        torch::autograd::AccumulateGrad         0.67%      18.779ms         1.33%      37.398ms      56.664us       9.188ms         0.30%      23.780ms      36.030us           660  \n",
      "                                 aten::nll_loss_forward         0.30%       8.594ms         0.30%       8.603ms       1.434ms       8.350ms         0.27%       8.439ms       1.407ms             6  \n",
      "                                             aten::add_         0.38%      10.836ms         0.38%      10.836ms      26.559us       8.136ms         0.27%       8.136ms      19.941us           408  \n",
      "                                               aten::le         0.37%      10.470ms         0.37%      10.470ms      24.577us       7.805ms         0.26%       7.805ms      18.322us           426  \n",
      "                                            aten::slice         0.30%       8.539ms         0.32%       8.904ms      35.474us       7.267ms         0.24%      10.047ms      40.028us           251  \n",
      "                                         aten::_to_copy         0.49%      13.729ms         0.96%      27.170ms     102.143us       6.974ms         0.23%      21.675ms      81.485us           266  \n",
      "    autograd::engine::evaluate_function: ClampBackward1         1.11%      31.446ms         8.09%     228.376ms     536.094us       6.817ms         0.22%     146.059ms     342.862us           426  \n",
      "      autograd::engine::evaluate_function: AddBackward0         0.57%      16.085ms         0.60%      16.904ms      32.383us       5.967ms         0.20%       8.941ms      17.128us           522  \n",
      "                                        aten::unsqueeze         0.20%       5.674ms         0.21%       5.949ms      59.490us       5.454ms         0.18%       7.515ms      75.150us           100  \n",
      "                          aten::max_pool2d_with_indices         0.17%       4.778ms         0.17%       4.778ms     796.333us       5.359ms         0.18%       5.359ms     893.167us             6  \n",
      "                                          aten::argsort         0.09%       2.489ms         5.20%     146.869ms       4.196ms       5.334ms         0.17%     148.582ms       4.245ms            35  \n",
      "                                            aten::relu_         0.26%       7.449ms         0.38%      10.824ms      54.667us       5.084ms         0.17%       9.335ms      47.146us           198  \n",
      "                                      aten::convolution         0.30%       8.371ms        29.77%     840.050ms       3.889ms       4.909ms         0.16%     776.065ms       3.593ms           216  \n",
      "                                       aten::clamp_min_         0.12%       3.375ms         0.12%       3.375ms      17.045us       4.251ms         0.14%       4.251ms      21.470us           198  \n",
      "                                     aten::_convolution         0.25%       7.194ms        29.47%     831.679ms       3.850ms       4.234ms         0.14%     771.156ms       3.570ms           216  \n",
      "                                           aten::conv2d         0.21%       5.877ms        29.97%     845.927ms       3.916ms       4.015ms         0.13%     780.080ms       3.611ms           216  \n",
      "                 aten::max_pool2d_with_indices_backward         0.05%       1.519ms         0.06%       1.789ms     298.167us       3.463ms         0.11%       3.839ms     639.833us             6  \n",
      "autograd::engine::evaluate_function: CudnnBatchNormB...         0.34%       9.597ms         3.03%      85.627ms     396.421us       3.413ms         0.11%      63.575ms     294.329us           216  \n",
      "                                   ConvolutionBackward0         0.27%       7.567ms         6.43%     181.402ms     839.824us       3.228ms         0.11%     376.263ms       1.742ms           216  \n",
      "                                          ReluBackward0         0.25%       7.132ms         0.78%      21.968ms     110.949us       3.169ms         0.10%      18.742ms      94.657us           198  \n",
      "                                CudnnBatchNormBackward0         0.28%       8.031ms         2.69%      76.030ms     351.991us       2.997ms         0.10%      60.162ms     278.528us           216  \n",
      "                                           AddBackward0         0.03%     781.000us         0.03%     781.000us       1.610us       2.974ms         0.10%       2.974ms       6.132us           485  \n",
      "                                          ViewBackward0         0.21%       6.008ms         0.44%      12.511ms      56.356us       2.866ms         0.09%       6.140ms      27.658us           222  \n",
      "                                Optimizer.step#SGD.step         0.23%       6.431ms         1.52%      42.772ms       7.129ms       2.817ms         0.09%      43.662ms       7.277ms             6  \n",
      "                      Optimizer.zero_grad#SGD.zero_grad         0.10%       2.743ms         0.10%       2.743ms     457.167us       2.799ms         0.09%       2.799ms     466.500us             6  \n",
      "                                           aten::select         0.09%       2.480ms         0.10%       2.880ms      80.000us       2.558ms         0.08%       3.818ms     106.056us            36  \n",
      "                                    aten::empty_strided         0.06%       1.678ms         0.06%       1.678ms       6.380us       2.526ms         0.08%       2.526ms       9.605us           263  \n",
      "autograd::engine::evaluate_function: ConvolutionBack...         0.29%       8.303ms         6.72%     189.705ms     878.264us       2.433ms         0.08%     378.696ms       1.753ms           216  \n",
      "                           aten::_batch_norm_impl_index         0.21%       5.954ms         2.89%      81.426ms     376.972us       2.428ms         0.08%      78.411ms     363.014us           216  \n",
      "     autograd::engine::evaluate_function: ReluBackward0         0.22%       6.348ms         1.00%      28.316ms     143.010us       2.359ms         0.08%      21.101ms     106.571us           198  \n",
      "                                       aten::batch_norm         0.18%       4.983ms         3.06%      86.409ms     400.042us       2.241ms         0.07%      80.652ms     373.389us           216  \n",
      "     autograd::engine::evaluate_function: ViewBackward0         0.20%       5.688ms         0.64%      18.199ms      81.977us       2.220ms         0.07%       8.360ms      37.658us           222  \n",
      "                                          aten::movedim         0.07%       1.991ms         0.14%       3.936ms     109.333us       1.813ms         0.06%       3.962ms     110.056us            36  \n",
      "                                                aten::t         0.07%       1.854ms         0.11%       2.990ms      99.667us       1.642ms         0.05%       2.911ms      97.033us            30  \n",
      "                                          aten::permute         0.06%       1.704ms         0.07%       1.945ms      54.028us       1.603ms         0.05%       2.149ms      59.694us            36  \n",
      "                                               aten::mm         0.06%       1.642ms         0.06%       1.642ms     136.833us       1.582ms         0.05%       1.582ms     131.833us            12  \n",
      "                                             aten::full         0.04%       1.085ms         0.22%       6.212ms     776.500us       1.481ms         0.05%       6.578ms     822.250us             8  \n",
      "                                           aten::linear         0.06%       1.659ms         0.81%      22.740ms       3.790ms       1.474ms         0.05%      22.004ms       3.667ms             6  \n",
      "                                aten::nll_loss_backward         0.05%       1.441ms         0.06%       1.676ms     279.333us       1.448ms         0.05%       1.622ms     270.333us             6  \n",
      "                                         aten::moveaxis         0.06%       1.652ms         0.20%       5.588ms     155.222us       1.399ms         0.05%       5.361ms     148.917us            36  \n",
      "                               aten::cross_entropy_loss         0.05%       1.550ms         1.12%      31.716ms       5.286ms       1.384ms         0.05%      31.340ms       5.223ms             6  \n",
      "                       aten::_log_softmax_backward_data         0.05%       1.362ms         0.05%       1.362ms     227.000us       1.340ms         0.04%       1.340ms     223.333us             6  \n",
      "                                         AddmmBackward0         0.07%       1.967ms         0.18%       5.153ms     858.833us       1.208ms         0.04%       4.186ms     697.667us             6  \n",
      "                                      aten::log_softmax         0.04%       1.204ms         0.73%      20.620ms       3.437ms       1.200ms         0.04%      20.580ms       3.430ms             6  \n",
      "                                    LogSoftmaxBackward0         0.04%       1.248ms         0.09%       2.610ms     435.000us       1.115ms         0.04%       2.455ms     409.167us             6  \n",
      "                                      aten::linalg_norm         0.08%       2.249ms         1.90%      53.751ms       1.075ms       1.054ms         0.03%     300.163ms       6.003ms            50  \n",
      "                                           aten::expand         0.03%     977.000us         0.04%       1.003ms      83.583us       1.019ms         0.03%       1.118ms      93.167us            12  \n",
      "                                       NllLossBackward0         0.04%       1.071ms         0.10%       2.747ms     457.833us       1.019ms         0.03%       2.641ms     440.167us             6  \n",
      "                                          MeanBackward1         0.02%     652.000us         0.50%      14.096ms       2.349ms       1.016ms         0.03%      14.060ms       2.343ms             6  \n",
      "                          MaxPool2DWithIndicesBackward0         0.04%       1.024ms         0.10%       2.813ms     468.833us     866.000us         0.03%       4.705ms     784.167us             6  \n",
      "                                        aten::transpose         0.04%       1.081ms         0.04%       1.136ms      37.867us     827.000us         0.03%       1.269ms      42.300us            30  \n",
      "                                       aten::max_pool2d         0.03%     800.000us         0.20%       5.578ms     929.667us     696.000us         0.02%       6.055ms       1.009ms             6  \n",
      "                                         aten::nll_loss         0.02%     595.000us         0.33%       9.198ms       1.533ms     547.000us         0.02%       8.986ms       1.498ms             6  \n",
      "                                          aten::flatten         0.02%     684.000us         0.03%     737.000us     122.833us     502.000us         0.02%     546.000us      91.000us             6  \n",
      "                              aten::_local_scalar_dense         0.32%       9.022ms         0.32%       9.022ms       1.504ms     490.000us         0.02%     490.000us      81.667us             6  \n",
      "autograd::engine::evaluate_function: NllLossBackward...         0.01%     291.000us         0.11%       3.038ms     506.333us     448.000us         0.01%       3.089ms     514.833us             6  \n",
      "                                      aten::nll_loss_nd         0.01%     348.000us         0.34%       9.546ms       1.591ms     390.000us         0.01%       9.376ms       1.563ms             6  \n",
      "                                             aten::mean         1.07%      30.177ms         1.07%      30.177ms       5.029ms     378.000us         0.01%     378.000us      63.000us             6  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.03%     755.000us         1.32%      37.240ms       6.207ms     368.000us         0.01%      35.946ms       5.991ms             6  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.822s\n",
      "Self CUDA time total: 3.051s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\"\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Dummy data\n",
    "inputs = torch.randn(256, 1, 28, 28).to(device)  # Batch size 64, MNIST-like input\n",
    "targets = torch.randint(0, 10, (256,)).to(device)  # Random target classes\n",
    "profile_training_step(model, optimizer, criterion, inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from custom_bnn import CBNNConv2d\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 =CBNNConv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = CBNNConv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet34, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = CBNNConv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Assuming grayscale input\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 3)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 3, stride=2)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                CBNNConv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ResNet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from bnn import BNNConv2d\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 =BNNConv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = BNNConv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet34(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ResNet34, self).__init__()\n",
    "        self.in_channels = 64\n",
    "\n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = BNNConv2d(1, 64, kernel_size=7, stride=2, padding=3)  # Assuming grayscale input\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(BasicBlock, 64, 3)\n",
    "        self.layer2 = self._make_layer(BasicBlock, 128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(BasicBlock, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(BasicBlock, 512, 3, stride=2)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * BasicBlock.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                BNNConv2d(self.in_channels, out_channels * block.expansion, kernel_size=1, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_channels, out_channels))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
